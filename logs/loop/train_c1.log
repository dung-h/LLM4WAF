/mnt/c/Users/HAD/Desktop/AI_in_cyber/LLM_in_Cyber/.venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1025: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/mnt/c/Users/HAD/Desktop/AI_in_cyber/LLM_in_Cyber/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
[DEBUG] Checking token path: /home/kali/.cache/huggingface/token
[DEBUG] Path exists: True
✅ Loaded HF_TOKEN from cache (len=37)
Loading tokenizer…
Loading 4-bit model… (this may take a while)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:59<00:59, 59.09s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 26.13s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 31.07s/it]
Preparing k-bit training and applying DoRA adapters…

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 269 examples [00:00, 4092.98 examples/s]

Map:   0%|          | 0/269 [00:00<?, ? examples/s]
Map: 100%|██████████| 269/269 [00:00<00:00, 2648.93 examples/s]
/mnt/c/Users/HAD/Desktop/AI_in_cyber/LLM_in_Cyber/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
/mnt/c/Users/HAD/Desktop/AI_in_cyber/LLM_in_Cyber/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/mnt/c/Users/HAD/Desktop/AI_in_cyber/LLM_in_Cyber/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
Initializing SFTTrainer…

Map:   0%|          | 0/269 [00:00<?, ? examples/s]
Map: 100%|██████████| 269/269 [00:00<00:00, 3486.61 examples/s]
/mnt/c/Users/HAD/Desktop/AI_in_cyber/LLM_in_Cyber/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:413: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.
Starting training… (ctrl+c to stop)

  0%|          | 0/60 [00:00<?, ?it/s]/mnt/c/Users/HAD/Desktop/AI_in_cyber/LLM_in_Cyber/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)

  2%|▏         | 1/60 [00:13<13:00, 13.22s/it]
  3%|▎         | 2/60 [00:25<12:07, 12.55s/it]
  5%|▌         | 3/60 [00:36<11:31, 12.12s/it]
  7%|▋         | 4/60 [00:49<11:31, 12.34s/it]
  8%|▊         | 5/60 [01:01<11:09, 12.17s/it]
 10%|█         | 6/60 [01:13<10:52, 12.08s/it]
 12%|█▏        | 7/60 [01:24<10:20, 11.70s/it]
 13%|█▎        | 8/60 [01:36<10:09, 11.73s/it]
 15%|█▌        | 9/60 [01:48<10:15, 12.08s/it]
 17%|█▋        | 10/60 [02:00<09:59, 11.99s/it]
                                               

 17%|█▋        | 10/60 [02:00<09:59, 11.99s/it]
 18%|█▊        | 11/60 [02:12<09:41, 11.87s/it]
 20%|██        | 12/60 [02:24<09:29, 11.86s/it]
 22%|██▏       | 13/60 [02:35<09:05, 11.60s/it]
 23%|██▎       | 14/60 [02:46<08:49, 11.52s/it]
 25%|██▌       | 15/60 [02:57<08:33, 11.41s/it]
 27%|██▋       | 16/60 [03:09<08:22, 11.43s/it]
 28%|██▊       | 17/60 [03:20<08:09, 11.39s/it]
 30%|███       | 18/60 [03:31<07:54, 11.29s/it]
 32%|███▏      | 19/60 [03:42<07:38, 11.19s/it]
 33%|███▎      | 20/60 [03:53<07:27, 11.19s/it]
                                               

 33%|███▎      | 20/60 [03:53<07:27, 11.19s/it]
 35%|███▌      | 21/60 [04:04<07:17, 11.22s/it]
 37%|███▋      | 22/60 [04:16<07:10, 11.33s/it]
 38%|███▊      | 23/60 [04:27<06:52, 11.14s/it]
 40%|████      | 24/60 [04:38<06:40, 11.12s/it]
 42%|████▏     | 25/60 [04:49<06:28, 11.10s/it]
 43%|████▎     | 26/60 [05:00<06:20, 11.19s/it]
 45%|████▌     | 27/60 [05:12<06:10, 11.22s/it]
 47%|████▋     | 28/60 [05:23<05:58, 11.21s/it]
 48%|████▊     | 29/60 [05:34<05:46, 11.17s/it]
 50%|█████     | 30/60 [05:45<05:37, 11.25s/it]
                                               

 50%|█████     | 30/60 [05:45<05:37, 11.25s/it]
 52%|█████▏    | 31/60 [05:56<05:23, 11.16s/it]
 53%|█████▎    | 32/60 [06:07<05:09, 11.07s/it]
 55%|█████▌    | 33/60 [06:18<04:59, 11.08s/it]
 57%|█████▋    | 34/60 [06:25<04:16,  9.88s/it]
 58%|█████▊    | 35/60 [06:36<04:15, 10.21s/it]
 60%|██████    | 36/60 [06:47<04:11, 10.49s/it]
 62%|██████▏   | 37/60 [06:58<04:04, 10.65s/it]
 63%|██████▎   | 38/60 [07:09<03:55, 10.72s/it]
 65%|██████▌   | 39/60 [07:20<03:47, 10.81s/it]
 67%|██████▋   | 40/60 [07:32<03:41, 11.10s/it]
                                               

 67%|██████▋   | 40/60 [07:32<03:41, 11.10s/it]
 68%|██████▊   | 41/60 [07:43<03:32, 11.19s/it]
 70%|███████   | 42/60 [07:54<03:19, 11.07s/it]
 72%|███████▏  | 43/60 [08:05<03:08, 11.07s/it]
 73%|███████▎  | 44/60 [08:18<03:05, 11.62s/it]
 75%|███████▌  | 45/60 [08:29<02:50, 11.36s/it]
 77%|███████▋  | 46/60 [08:39<02:34, 11.06s/it]
 78%|███████▊  | 47/60 [08:51<02:26, 11.25s/it]
 80%|████████  | 48/60 [09:03<02:16, 11.35s/it]
 82%|████████▏ | 49/60 [09:14<02:05, 11.40s/it]
 83%|████████▎ | 50/60 [09:25<01:53, 11.36s/it]
                                               

 83%|████████▎ | 50/60 [09:25<01:53, 11.36s/it]
 85%|████████▌ | 51/60 [09:36<01:41, 11.28s/it]
 87%|████████▋ | 52/60 [09:47<01:28, 11.07s/it]
 88%|████████▊ | 53/60 [09:59<01:18, 11.19s/it]
 90%|█████████ | 54/60 [10:11<01:08, 11.46s/it]
 92%|█████████▏| 55/60 [10:22<00:56, 11.35s/it]
 93%|█████████▎| 56/60 [10:34<00:46, 11.62s/it]
 95%|█████████▌| 57/60 [10:47<00:35, 11.92s/it]
 97%|█████████▋| 58/60 [10:57<00:23, 11.59s/it]
 98%|█████████▊| 59/60 [11:08<00:11, 11.36s/it]
100%|██████████| 60/60 [11:19<00:00, 11.13s/it]
                                               

100%|██████████| 60/60 [11:19<00:00, 11.13s/it]
                                               

100%|██████████| 60/60 [11:24<00:00, 11.13s/it]
100%|██████████| 60/60 [11:24<00:00, 11.41s/it]
{'loss': 2.0469, 'grad_norm': 2.5771777629852295, 'learning_rate': 0.00019289767198167916, 'epoch': 0.3}
{'loss': 0.5692, 'grad_norm': 1.8068310022354126, 'learning_rate': 0.00016051742151937655, 'epoch': 0.59}
{'loss': 0.3758, 'grad_norm': 2.0199673175811768, 'learning_rate': 0.00011081190184239419, 'epoch': 0.89}
{'loss': 0.2807, 'grad_norm': 1.133170485496521, 'learning_rate': 5.801108984397354e-05, 'epoch': 1.18}
{'loss': 0.2432, 'grad_norm': 0.700513482093811, 'learning_rate': 1.7231100184310956e-05, 'epoch': 1.48}
{'loss': 0.2024, 'grad_norm': 0.7378008961677551, 'learning_rate': 1.4665861488761813e-07, 'epoch': 1.77}
{'train_runtime': 684.6334, 'train_samples_per_second': 0.701, 'train_steps_per_second': 0.088, 'train_loss': 0.6197016954421997, 'epoch': 1.77}
Saving adapter…
Done.
