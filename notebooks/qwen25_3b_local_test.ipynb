{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec6d0cb",
   "metadata": {},
   "source": [
    "# Qwen2.5-3B LoRA Local Test Setup\n",
    "\n",
    "**Goal**: Test Qwen2.5-3B-Instruct LoRA fine-tuning on local machine before scaling to Kaggle\n",
    "\n",
    "**Key Benefits of Qwen2.5-3B**:\n",
    "- 3B parameters (vs 7B) ‚Üí Fits 8GB RAM easily\n",
    "- Qwen2.5 series (latest) ‚Üí Better quality, better optimization\n",
    "- Instruct version ‚Üí Pre-trained for chat format, faster convergence\n",
    "- ~5-10 min per epoch on local GPU vs 30+ min for 7B\n",
    "\n",
    "**Execution plan**:\n",
    "1. Install minimal dependencies (check versions)\n",
    "2. Detect local GPU/CPU capacity\n",
    "3. Load Qwen2.5-3B-Instruct with 4-bit quantization\n",
    "4. Create tiny dataset (100 samples) for quick test\n",
    "5. Train 1 epoch to verify everything works\n",
    "6. Test inference\n",
    "7. Document any issues before remote deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f7166",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Dependencies and Version Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7497ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 00:37:36,050 - INFO - üöÄ Starting Qwen2.5-3B local test setup\n",
      "2025-11-27 00:37:36,053 - INFO - üîë Checking HuggingFace token...\n",
      "2025-11-27 00:37:36,053 - INFO - üîë Checking HuggingFace token...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è HF_TOKEN not found in environment\n",
      "Set it with: export HF_TOKEN=your_token\n",
      "Or modify this cell to add it\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 00:38:05,920 - INFO - ‚úÖ HF token set (37 chars)\n",
      "2025-11-27 00:38:05,922 - INFO -   [1/8] Installing torch==2.1.2...\n",
      "2025-11-27 00:38:05,922 - INFO -   [1/8] Installing torch==2.1.2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HF token ready\n",
      "\n",
      "üì¶ Installing dependencies...\n",
      "\n",
      "  ‚Üí torch==2.1.2 ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cell-gears 0.0.2 requires scanpy, which is not installed.\n",
      "bitsandbytes 0.48.2 requires torch<3,>=2.3, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cell-gears 0.0.2 requires scanpy, which is not installed.\n",
      "bitsandbytes 0.48.2 requires torch<3,>=2.3, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m2025-11-27 00:43:40,882 - INFO -   ‚úì torch==2.1.2\n",
      "2025-11-27 00:43:40,886 - INFO -   [2/8] Installing transformers>=4.40.0...\n",
      "2025-11-27 00:43:40,882 - INFO -   ‚úì torch==2.1.2\n",
      "2025-11-27 00:43:40,886 - INFO -   [2/8] Installing transformers>=4.40.0...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì\n",
      "  ‚Üí transformers>=4.40.0 ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m2025-11-27 00:43:43,175 - INFO -   ‚úì transformers>=4.40.0\n",
      "2025-11-27 00:43:43,179 - INFO -   [3/8] Installing peft==0.7.1...\n",
      "2025-11-27 00:43:43,175 - INFO -   ‚úì transformers>=4.40.0\n",
      "2025-11-27 00:43:43,179 - INFO -   [3/8] Installing peft==0.7.1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì\n",
      "  ‚Üí peft==0.7.1 ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m2025-11-27 00:43:45,900 - INFO -   ‚úì peft==0.7.1\n",
      "2025-11-27 00:43:45,902 - INFO -   [4/8] Installing bitsandbytes==0.41.3.post2...\n",
      "2025-11-27 00:43:45,900 - INFO -   ‚úì peft==0.7.1\n",
      "2025-11-27 00:43:45,902 - INFO -   [4/8] Installing bitsandbytes==0.41.3.post2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì\n",
      "  ‚Üí bitsandbytes==0.41.3.post2 ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m2025-11-27 00:44:01,034 - INFO -   ‚úì bitsandbytes==0.41.3.post2\n",
      "2025-11-27 00:44:01,035 - INFO -   [5/8] Installing safetensors...\n",
      "2025-11-27 00:44:01,034 - INFO -   ‚úì bitsandbytes==0.41.3.post2\n",
      "2025-11-27 00:44:01,035 - INFO -   [5/8] Installing safetensors...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì\n",
      "  ‚Üí safetensors ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m2025-11-27 00:44:03,071 - INFO -   ‚úì safetensors\n",
      "2025-11-27 00:44:03,073 - INFO -   [6/8] Installing huggingface-hub...\n",
      "2025-11-27 00:44:03,071 - INFO -   ‚úì safetensors\n",
      "2025-11-27 00:44:03,073 - INFO -   [6/8] Installing huggingface-hub...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì\n",
      "  ‚Üí huggingface-hub ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m2025-11-27 00:44:05,415 - INFO -   ‚úì huggingface-hub\n",
      "2025-11-27 00:44:05,417 - INFO -   [7/8] Installing datasets==2.14.5...\n",
      "2025-11-27 00:44:05,415 - INFO -   ‚úì huggingface-hub\n",
      "2025-11-27 00:44:05,417 - INFO -   [7/8] Installing datasets==2.14.5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì\n",
      "  ‚Üí datasets==2.14.5 ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m2025-11-27 00:44:12,045 - INFO -   ‚úì datasets==2.14.5\n",
      "2025-11-27 00:44:12,047 - INFO -   [8/8] Installing accelerate==0.24.1...\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m2025-11-27 00:44:12,045 - INFO -   ‚úì datasets==2.14.5\n",
      "2025-11-27 00:44:12,047 - INFO -   [8/8] Installing accelerate==0.24.1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì\n",
      "  ‚Üí accelerate==0.24.1 ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m2025-11-27 00:44:14,899 - INFO -   ‚úì accelerate==0.24.1\n",
      "2025-11-27 00:44:14,901 - INFO - ‚úÖ Installed 8/8\n",
      "2025-11-27 00:44:14,903 - INFO - \n",
      "üìã Version Check:\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/home/kali/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m2025-11-27 00:44:14,899 - INFO -   ‚úì accelerate==0.24.1\n",
      "2025-11-27 00:44:14,901 - INFO - ‚úÖ Installed 8/8\n",
      "2025-11-27 00:44:14,903 - INFO - \n",
      "üìã Version Check:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì\n",
      "\n",
      "‚úÖ Installed 8/8 packages\n",
      "\n",
      "üìã Version Check:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 00:44:17,575 - INFO -   ‚Ä¢ torch: 2.1.2+cu121\n",
      "/home/kali/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/kali/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Ä¢ torch: 2.1.2+cu121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 00:44:18,130 - INFO -   ‚Ä¢ transformers: 4.44.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Ä¢ transformers: 4.44.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 00:44:21,448 - INFO -   ‚Ä¢ peft: 0.7.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Ä¢ peft: 0.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 00:44:25,140 - WARNING -   ‚úó bitsandbytes not available\n",
      "2025-11-27 00:44:25,160 - INFO - ‚úÖ Qwen2Tokenizer available\n",
      "2025-11-27 00:44:25,163 - INFO - \n",
      "üìä Log file: qwen25_local_test_20251127_003736.log\n",
      "2025-11-27 00:44:25,160 - INFO - ‚úÖ Qwen2Tokenizer available\n",
      "2025-11-27 00:44:25,163 - INFO - \n",
      "üìä Log file: qwen25_local_test_20251127_003736.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Qwen2.5 Support Check:\n",
      "‚úÖ Qwen2 tokenizer support ready\n",
      "\n",
      "üìä Logging to: qwen25_local_test_20251127_003736.log\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# üìä SETUP LOGGING\n",
    "log_file = f\"qwen25_local_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"üöÄ Starting Qwen2.5-3B local test setup\")\n",
    "\n",
    "# üîê HuggingFace Token Setup\n",
    "logger.info(\"üîë Checking HuggingFace token...\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", None)\n",
    "if not HF_TOKEN:\n",
    "    print(\"‚ö†Ô∏è HF_TOKEN not found in environment\")\n",
    "    print(\"Set it with: export HF_TOKEN=your_token\")\n",
    "    print(\"Or modify this cell to add it\\n\")\n",
    "    HF_TOKEN = input(\"Enter HF token (or press Enter to skip): \").strip()\n",
    "    if HF_TOKEN:\n",
    "        os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "if HF_TOKEN:\n",
    "    logger.info(f\"‚úÖ HF token set ({len(HF_TOKEN)} chars)\")\n",
    "    print(f\"‚úÖ HF token ready\\n\")\n",
    "else:\n",
    "    logger.warning(\"‚ö†Ô∏è Proceeding without HF token (may fail for private models)\")\n",
    "    print(\"‚ö†Ô∏è Proceeding without HF token\\n\")\n",
    "\n",
    "# üì¶ INSTALL PACKAGES - with proper version compatibility\n",
    "print(\"üì¶ Installing dependencies...\\n\")\n",
    "packages = [\n",
    "    \"torch==2.1.2\",              # Compatible with bitsandbytes\n",
    "    \"transformers>=4.40.0\",      # Latest, has Qwen2.5 support\n",
    "    \"peft==0.7.1\",               # LoRA\n",
    "    \"bitsandbytes==0.41.3.post2\",# 4-bit quantization\n",
    "    \"safetensors>=0.4.0\",        # Model format - fixed version\n",
    "    \"huggingface-hub>=0.20.0\",   # HF auth - fixed version\n",
    "    \"pyarrow>=14.0.0\",           # ‚ö†Ô∏è CRITICAL: Must be >=14.0.0 for datasets compatibility\n",
    "    \"datasets>=2.15.0\",          # Fixed to avoid PyExtensionType error\n",
    "    \"accelerate==0.24.1\",        # GPU support\n",
    "]\n",
    "\n",
    "installed = []\n",
    "failed = []\n",
    "\n",
    "for i, package in enumerate(packages, 1):\n",
    "    try:\n",
    "        logger.info(f\"  [{i}/{len(packages)}] Installing {package}...\")\n",
    "        print(f\"  ‚Üí {package}\", end=\" ... \")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "        print(\"‚úì\")\n",
    "        installed.append(package)\n",
    "        logger.info(f\"  ‚úì {package}\")\n",
    "    except Exception as e:\n",
    "        print(\"‚úó\")\n",
    "        logger.error(f\"  ‚úó {package}: {e}\")\n",
    "        failed.append((package, str(e)))\n",
    "\n",
    "logger.info(f\"‚úÖ Installed {len(installed)}/{len(packages)}\")\n",
    "print(f\"\\n‚úÖ Installed {len(installed)}/{len(packages)} packages\")\n",
    "\n",
    "if failed:\n",
    "    logger.warning(f\"‚ö†Ô∏è {len(failed)} packages failed to install:\")\n",
    "    print(f\"‚ö†Ô∏è {len(failed)} packages failed:\")\n",
    "    for pkg, err in failed:\n",
    "        print(f\"   ‚Ä¢ {pkg}: {err[:100]}\")\n",
    "        logger.warning(f\"   ‚Ä¢ {pkg}: {err[:100]}\")\n",
    "\n",
    "# üìã VERIFY VERSIONS\n",
    "logger.info(\"\\nüìã Version Check:\")\n",
    "print(\"\\nüìã Version Check:\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    logger.info(f\"  ‚Ä¢ torch: {torch.__version__}\")\n",
    "    print(f\"  ‚Ä¢ torch: {torch.__version__}\")\n",
    "except:\n",
    "    logger.warning(\"  ‚úó torch not available\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    logger.info(f\"  ‚Ä¢ transformers: {transformers.__version__}\")\n",
    "    print(f\"  ‚Ä¢ transformers: {transformers.__version__}\")\n",
    "except:\n",
    "    logger.warning(\"  ‚úó transformers not available\")\n",
    "\n",
    "try:\n",
    "    import peft\n",
    "    logger.info(f\"  ‚Ä¢ peft: {peft.__version__}\")\n",
    "    print(f\"  ‚Ä¢ peft: {peft.__version__}\")\n",
    "except:\n",
    "    logger.warning(\"  ‚úó peft not available\")\n",
    "\n",
    "try:\n",
    "    import bitsandbytes\n",
    "    logger.info(f\"  ‚Ä¢ bitsandbytes: {bitsandbytes.__version__}\")\n",
    "    print(f\"  ‚Ä¢ bitsandbytes: {bitsandbytes.__version__}\")\n",
    "except:\n",
    "    logger.warning(\"  ‚úó bitsandbytes not available\")\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    logger.info(f\"  ‚Ä¢ pyarrow: {pa.__version__}\")\n",
    "    print(f\"  ‚Ä¢ pyarrow: {pa.__version__}\")\n",
    "except:\n",
    "    logger.warning(\"  ‚úó pyarrow not available\")\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    logger.info(f\"  ‚Ä¢ datasets: {datasets.__version__}\")\n",
    "    print(f\"  ‚Ä¢ datasets: {datasets.__version__}\")\n",
    "except:\n",
    "    logger.warning(\"  ‚úó datasets not available\")\n",
    "\n",
    "# ‚úÖ Verify Qwen2.5 tokenizer support\n",
    "print(\"\\n‚úÖ Qwen2.5 Support Check:\")\n",
    "try:\n",
    "    from transformers.models.qwen2 import Qwen2Tokenizer\n",
    "    logger.info(\"‚úÖ Qwen2Tokenizer available\")\n",
    "    print(\"‚úÖ Qwen2 tokenizer support ready\")\n",
    "except ImportError:\n",
    "    logger.warning(\"‚ö†Ô∏è Qwen2Tokenizer not found (will be installed with model)\")\n",
    "    print(\"‚ö†Ô∏è Qwen2Tokenizer not in cache (normal, loads with model)\")\n",
    "\n",
    "logger.info(f\"\\nüìä Log file: {log_file}\")\n",
    "print(f\"\\nüìä Logging to: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac918e8",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Device Detection and Memory Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff76fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 00:44:25,225 - INFO - \n",
      "üîç Device Detection and Memory Setup\n",
      "2025-11-27 00:44:25,229 - INFO - Device: cuda\n",
      "2025-11-27 00:44:25,231 - INFO - GPU: NVIDIA GeForce RTX 4060 Laptop GPU | VRAM: 8.59 GB\n",
      "2025-11-27 00:44:25,233 - INFO - Auto-detected: batch_size=2, grad_accum=1\n",
      "2025-11-27 00:44:25,235 - INFO - ‚úÖ GPU cache cleared\n",
      "2025-11-27 00:44:25,237 - INFO - ‚öôÔ∏è Configuring 4-bit quantization (NF4)\n",
      "2025-11-27 00:44:25,244 - INFO - ‚úÖ BitsAndBytes 4-bit config ready (NF4, double quant, bfloat16 compute)\n",
      "2025-11-27 00:44:25,229 - INFO - Device: cuda\n",
      "2025-11-27 00:44:25,231 - INFO - GPU: NVIDIA GeForce RTX 4060 Laptop GPU | VRAM: 8.59 GB\n",
      "2025-11-27 00:44:25,233 - INFO - Auto-detected: batch_size=2, grad_accum=1\n",
      "2025-11-27 00:44:25,235 - INFO - ‚úÖ GPU cache cleared\n",
      "2025-11-27 00:44:25,237 - INFO - ‚öôÔ∏è Configuring 4-bit quantization (NF4)\n",
      "2025-11-27 00:44:25,244 - INFO - ‚úÖ BitsAndBytes 4-bit config ready (NF4, double quant, bfloat16 compute)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Device Detection\n",
      "\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "VRAM: 8.59 GB\n",
      "\n",
      "Auto-detected batch size: 2\n",
      "Gradient accumulation: 1\n",
      "\n",
      "‚úÖ GPU cache cleared\n",
      "\n",
      "‚úÖ 4-bit quantization configured\n",
      "\n",
      "============================================================\n",
      "CONFIGURATION SUMMARY\n",
      "============================================================\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU (8.6 GB)\n",
      "Batch size: 2\n",
      "Gradient accumulation: 1\n",
      "Quantization: 4-bit NF4 (double quant)\n",
      "Compute dtype: bfloat16\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"\\nüîç Device Detection and Memory Setup\")\n",
    "print(\"\\nüîç Device Detection\\n\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.info(f\"Device: {device}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# GPU/CPU specific config\n",
    "if device == \"cuda\":\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    logger.info(f\"GPU: {gpu_name} | VRAM: {vram_gb:.2f} GB\")\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {vram_gb:.2f} GB\\n\")\n",
    "    \n",
    "    # Auto-detect batch size for Qwen2.5-3B\n",
    "    # 3B model needs ~3-4GB base + ~2GB per sample (4-bit quantized)\n",
    "    if vram_gb >= 40:\n",
    "        AUTO_BATCH_SIZE = 16\n",
    "        GRADIENT_ACCUMULATION = 2\n",
    "    elif vram_gb >= 24:\n",
    "        AUTO_BATCH_SIZE = 8\n",
    "        GRADIENT_ACCUMULATION = 2\n",
    "    elif vram_gb >= 16:\n",
    "        AUTO_BATCH_SIZE = 4\n",
    "        GRADIENT_ACCUMULATION = 1\n",
    "    elif vram_gb >= 8:\n",
    "        AUTO_BATCH_SIZE = 2\n",
    "        GRADIENT_ACCUMULATION = 1\n",
    "    else:\n",
    "        AUTO_BATCH_SIZE = 1\n",
    "        GRADIENT_ACCUMULATION = 1\n",
    "    \n",
    "    logger.info(f\"Auto-detected: batch_size={AUTO_BATCH_SIZE}, grad_accum={GRADIENT_ACCUMULATION}\")\n",
    "    print(f\"Auto-detected batch size: {AUTO_BATCH_SIZE}\")\n",
    "    print(f\"Gradient accumulation: {GRADIENT_ACCUMULATION}\\n\")\n",
    "    \n",
    "    # Memory optimization\n",
    "    torch.cuda.empty_cache()\n",
    "    logger.info(\"‚úÖ GPU cache cleared\")\n",
    "    print(\"‚úÖ GPU cache cleared\\n\")\n",
    "else:\n",
    "    logger.warning(\"‚ö†Ô∏è CPU-only mode detected - training will be VERY slow\")\n",
    "    print(\"‚ö†Ô∏è CPU-only mode - training will be slow\\n\")\n",
    "    AUTO_BATCH_SIZE = 1\n",
    "    GRADIENT_ACCUMULATION = 1\n",
    "\n",
    "# 4-bit quantization config (optimal for 3B models)\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "logger.info(\"‚öôÔ∏è Configuring 4-bit quantization (NF4)\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "logger.info(\"‚úÖ BitsAndBytes 4-bit config ready (NF4, double quant, bfloat16 compute)\")\n",
    "print(\"‚úÖ 4-bit quantization configured\\n\")\n",
    "\n",
    "# Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {gpu_name} ({vram_gb:.1f} GB)\")\n",
    "print(f\"Batch size: {AUTO_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"Quantization: 4-bit NF4 (double quant)\")\n",
    "print(f\"Compute dtype: bfloat16\")\n",
    "print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13068384",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Qwen2.5-3B-Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb317eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 00:44:25,276 - INFO - Loading tokenizer from Qwen/Qwen2.5-3B-Instruct...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING QWEN2.5-3B-INSTRUCT\n",
      "============================================================\n",
      "\n",
      "üì• Loading tokenizer: Qwen/Qwen2.5-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 00:44:31,297 - INFO - ‚úÖ Tokenizer loaded successfully\n",
      "2025-11-27 00:44:31,300 - INFO - Loading model with 4-bit quantization...\n",
      "2025-11-27 00:44:31,300 - INFO - Loading model with 4-bit quantization...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer ready | Vocab: 151643\n",
      "\n",
      "üì• Loading model (may take 2-3 minutes on first run)...\n",
      "   - Size: 3B parameters\n",
      "   - Quantization: 4-bit NF4\n",
      "   - Expected VRAM: ~3-4 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [05:55<00:00, 177.73s/it]\n",
      "2025-11-27 00:50:29,225 - WARNING - Flash attention 2 not available: FlashAttention2 has been toggled on, but it cannot be used due to the following \n",
      "Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [05:55<00:00, 177.73s/it]\n",
      "2025-11-27 00:50:29,225 - WARNING - Flash attention 2 not available: FlashAttention2 has been toggled on, but it cannot be used due to the following \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Flash attention 2 not available, using default\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 00:50:49,332 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:25<00:00, 12.88s/it]\n",
      "\n",
      "2025-11-27 00:51:17,662 - INFO - ‚úÖ Model loaded with default attention\n",
      "2025-11-27 00:51:17,669 - INFO - ‚úÖ Model loaded successfully\n",
      "2025-11-27 00:51:17,672 - INFO -   ‚Ä¢ Model: Qwen/Qwen2.5-3B-Instruct\n",
      "2025-11-27 00:51:17,675 - INFO -   ‚Ä¢ Parameters: 1.70B\n",
      "2025-11-27 00:51:17,677 - INFO -   ‚Ä¢ Attention: default\n",
      "2025-11-27 00:51:17,662 - INFO - ‚úÖ Model loaded with default attention\n",
      "2025-11-27 00:51:17,669 - INFO - ‚úÖ Model loaded successfully\n",
      "2025-11-27 00:51:17,672 - INFO -   ‚Ä¢ Model: Qwen/Qwen2.5-3B-Instruct\n",
      "2025-11-27 00:51:17,675 - INFO -   ‚Ä¢ Parameters: 1.70B\n",
      "2025-11-27 00:51:17,677 - INFO -   ‚Ä¢ Attention: default\n",
      "2025-11-27 00:51:17,682 - INFO -   ‚Ä¢ Dtype: torch.bfloat16\n",
      "2025-11-27 00:51:17,685 - INFO -   ‚Ä¢ Device: cuda:0\n",
      "2025-11-27 00:51:17,682 - INFO -   ‚Ä¢ Dtype: torch.bfloat16\n",
      "2025-11-27 00:51:17,685 - INFO -   ‚Ä¢ Device: cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "‚úÖ MODEL LOADED\n",
      "============================================================\n",
      "Model: Qwen/Qwen2.5-3B-Instruct\n",
      "Parameters: 1.70B\n",
      "Attention: default\n",
      "Dtype: torch.bfloat16\n",
      "Device: cuda:0\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING QWEN2.5-3B-INSTRUCT\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Model selection: Instruct version (pre-tuned for chat)\n",
    "model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "hf_token = os.getenv(\"HF_TOKEN\", None)\n",
    "\n",
    "logger.info(f\"Loading tokenizer from {model_id}...\")\n",
    "print(f\"üì• Loading tokenizer: {model_id}\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=True,\n",
    "        token=hf_token,\n",
    "    )\n",
    "    logger.info(f\"‚úÖ Tokenizer loaded successfully\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"‚úÖ Tokenizer ready | Vocab: {tokenizer.vocab_size}\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Failed to load tokenizer: {e}\")\n",
    "    print(f\"‚ùå Failed to load tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "logger.info(f\"Loading model with 4-bit quantization...\")\n",
    "print(f\"üì• Loading model (may take 2-3 minutes on first run)...\")\n",
    "print(f\"   - Size: 3B parameters\")\n",
    "print(f\"   - Quantization: 4-bit NF4\")\n",
    "print(f\"   - Expected VRAM: ~3-4 GB\\n\")\n",
    "\n",
    "try:\n",
    "    # Try with flash_attention_2 first (faster)\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            token=hf_token,\n",
    "        )\n",
    "        logger.info(f\"‚úÖ Model loaded with flash_attention_2\")\n",
    "        attention_type = \"flash_attention_2 (optimized)\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Flash attention 2 not available: {str(e)[:80]}\")\n",
    "        print(\"‚ö†Ô∏è Flash attention 2 not available, using default\\n\")\n",
    "        \n",
    "        # Fallback to default attention\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            token=hf_token,\n",
    "        )\n",
    "        logger.info(f\"‚úÖ Model loaded with default attention\")\n",
    "        attention_type = \"default\"\n",
    "    \n",
    "    # Model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    logger.info(f\"‚úÖ Model loaded successfully\")\n",
    "    logger.info(f\"  ‚Ä¢ Model: {model_id}\")\n",
    "    logger.info(f\"  ‚Ä¢ Parameters: {total_params / 1e9:.2f}B\")\n",
    "    logger.info(f\"  ‚Ä¢ Attention: {attention_type}\")\n",
    "    logger.info(f\"  ‚Ä¢ Dtype: {model.dtype}\")\n",
    "    logger.info(f\"  ‚Ä¢ Device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    print(f\"=\" * 60)\n",
    "    print(\"‚úÖ MODEL LOADED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Model: {model_id}\")\n",
    "    print(f\"Parameters: {total_params / 1e9:.2f}B\")\n",
    "    print(f\"Attention: {attention_type}\")\n",
    "    print(f\"Dtype: {model.dtype}\")\n",
    "    print(f\"Device: {next(model.parameters()).device}\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Model loading failed: {e}\", exc_info=True)\n",
    "    print(f\"‚ùå Model loading failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f365e84",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Create Tiny Local Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73da7f9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow' has no attribute 'PyExtensionType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m      4\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/__init__.py:22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,g-bad-import-order,wrong-import-position\u001b[39;00m\n\u001b[1;32m     20\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.14.5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:67\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_files\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sanitize_patterns\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_writer.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Features, Image, Value\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     FeatureType,\n\u001b[1;32m     30\u001b[0m     _ArrayXDExtensionType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     to_pyarrow_listarray,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilesystems\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_remote_filesystem\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/features/__init__.py:18\u001b[0m\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArray2D\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslationVariableLanguages\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m ]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Array2D, Array3D, Array4D, Array5D, ClassLabel, Features, Sequence, Value\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Translation, TranslationVariableLanguages\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/features/features.py:634\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;66;03m# Automatically constructed\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     _type: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m field(default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArray5D\u001b[39m\u001b[38;5;124m\"\u001b[39m, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mrepr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 634\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_ArrayXDExtensionType\u001b[39;00m(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyExtensionType\u001b[49m):\n\u001b[1;32m    635\u001b[0m     ndims: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, shape: \u001b[38;5;28mtuple\u001b[39m, dtype: \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow' has no attribute 'PyExtensionType'"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"üìù Creating tiny test dataset (100 samples)\\n\")\n",
    "\n",
    "# Synthetic WAF evasion training data\n",
    "synthetic_samples = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Generate a SQL injection payload that evades basic WAF\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"SELECT/**/1/**/FROM/**/users\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is a common XSS payload?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"<script>alert(1)</script>\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"How to bypass ModSecurity WAF?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Use case sensitivity: sElEcT or encoded payloads\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Generate SQLi with encoding\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"UNION/**/SELECT/**/table_name/**/FROM/**/information_schema.tables\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What are WAF bypass techniques?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Comment insertion, case manipulation, encoding, null bytes, encoding\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Repeat to create 100 samples\n",
    "test_dataset = synthetic_samples * 20  # 5 * 20 = 100\n",
    "\n",
    "logger.info(f\"‚úÖ Created {len(test_dataset)} synthetic samples\")\n",
    "print(f\"‚úÖ Generated {len(test_dataset)} training samples\")\n",
    "print(f\"   Each epoch: 100 samples\")\n",
    "print(f\"   Training time: ~1-2 minutes\\n\")\n",
    "\n",
    "# Format to Qwen chat format\n",
    "def convert_to_qwen_format(record):\n",
    "    \"\"\"Convert to Qwen2.5 exact chat format\"\"\"\n",
    "    if \"messages\" not in record:\n",
    "        return \"\"\n",
    "    \n",
    "    messages = record[\"messages\"]\n",
    "    if not messages or len(messages) < 2:\n",
    "        return \"\"\n",
    "    \n",
    "    # System + user + assistant format\n",
    "    text = \"<|im_start|>system\\nYou are a helpful assistant specialized in cybersecurity and WAF techniques.\\n<|im_end|>\\n\"\n",
    "    \n",
    "    for msg in messages:\n",
    "        role = msg.get(\"role\", \"\").lower()\n",
    "        content = msg.get(\"content\", \"\").strip()\n",
    "        \n",
    "        if not content or role not in [\"user\", \"assistant\"]:\n",
    "            continue\n",
    "        \n",
    "        text += f\"<|im_start|>{role}\\n{content}\\n<|im_end|>\\n\"\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Convert all samples\n",
    "logger.info(\"Converting dataset to Qwen2.5 format...\")\n",
    "print(\"Converting dataset to Qwen2.5 format...\")\n",
    "\n",
    "formatted_texts = []\n",
    "for record in test_dataset:\n",
    "    text = convert_to_qwen_format(record)\n",
    "    if text:\n",
    "        formatted_texts.append({\"text\": text})\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": [t[\"text\"] for t in formatted_texts]})\n",
    "\n",
    "logger.info(f\"‚úÖ Dataset ready: {len(dataset)} samples\")\n",
    "print(f\"‚úÖ Dataset ready: {len(dataset)} samples\\n\")\n",
    "\n",
    "# Show sample\n",
    "print(\"üìã Sample formatted text:\")\n",
    "print(\"=\" * 60)\n",
    "print(formatted_texts[0][\"text\"][:300])\n",
    "print(\"...\" + \"\\n\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e50375",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Tokenize and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca03b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"\\nüîÑ Tokenizing dataset...\\n\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize and prepare data for training\"\"\"\n",
    "    max_seq_length = 512  # Smaller for 3B model\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    \n",
    "    # Set labels = input_ids for causal language modeling\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "logger.info(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    remove_columns=[\"text\"],\n",
    "    desc=\"Tokenizing\",\n",
    ")\n",
    "\n",
    "logger.info(f\"‚úÖ Tokenization complete\")\n",
    "print(f\"‚úÖ Tokenization complete\")\n",
    "print(f\"   ‚Ä¢ Samples: {len(tokenized_dataset)}\")\n",
    "print(f\"   ‚Ä¢ Max sequence length: 512\")\n",
    "print(f\"   ‚Ä¢ Sample token length: {len(tokenized_dataset[0]['input_ids'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86b6a0",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Configure LoRA for Qwen2.5-3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab7830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚öôÔ∏è Setting up LoRA for Qwen2.5-3B\\n\")\n",
    "\n",
    "# LoRA config optimized for 3B model\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Smaller rank for 3B (vs 16 for 7B)\n",
    "    lora_alpha=16,  # Smaller alpha\n",
    "    target_modules=[\n",
    "        \"q_proj\",      # Query projection (Attention)\n",
    "        \"v_proj\",      # Value projection (Attention)\n",
    "        \"k_proj\",      # Key projection (Attention)\n",
    "        \"o_proj\",      # Output projection (Attention)\n",
    "        \"gate_proj\",   # Gate projection (MLP)\n",
    "        \"up_proj\",     # Up projection (MLP)\n",
    "        \"down_proj\",   # Down projection (MLP)\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "logger.info(\"Verifying LoRA target modules...\")\n",
    "model_params = set()\n",
    "for name, _ in model.named_parameters():\n",
    "    for target in lora_config.target_modules:\n",
    "        if target in name:\n",
    "            model_params.add(target)\n",
    "\n",
    "logger.info(f\"‚úì Found target modules: {sorted(model_params)}\")\n",
    "\n",
    "# Wrap with LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "logger.info(\"‚úÖ LoRA adapter configured for Qwen2.5-3B\")\n",
    "logger.info(f\"  ‚Ä¢ Rank: {lora_config.r}\")\n",
    "logger.info(f\"  ‚Ä¢ Alpha: {lora_config.lora_alpha}\")\n",
    "logger.info(f\"  ‚Ä¢ Target modules: {len(lora_config.target_modules)} layers\")\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "trainable_pct = trainable / total * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ LoRA Configuration (Qwen2.5-3B)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Rank (r): {lora_config.r} (smaller for 3B model)\")\n",
    "print(f\"Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"Target modules: {len(lora_config.target_modules)} layers\")\n",
    "print(f\"  ‚Ä¢ Attention: q_proj, v_proj, k_proj, o_proj\")\n",
    "print(f\"  ‚Ä¢ MLP: gate_proj, up_proj, down_proj\")\n",
    "print()\n",
    "print(f\"Trainable parameters: {trainable / 1e6:.2f}M ({trainable_pct:.2f}%)\")\n",
    "print(f\"Total parameters: {total / 1e9:.2f}B\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "logger.info(f\"‚úÖ Model wrapped with LoRA\")\n",
    "logger.info(f\"  ‚Ä¢ Trainable: {trainable / 1e6:.2f}M ({trainable_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb0b26",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Quick Training Test (1 Epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf80c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"QUICK TRAINING TEST\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"qwen25_local_adapter_test\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Training config - minimal for quick test\n",
    "NUM_EPOCHS = 1\n",
    "TRAIN_BATCH_SIZE = AUTO_BATCH_SIZE\n",
    "GRADIENT_ACCUMULATION = GRADIENT_ACCUMULATION\n",
    "\n",
    "logger.info(\"üîß Training Configuration\")\n",
    "logger.info(f\"  ‚Ä¢ Epochs: {NUM_EPOCHS}\")\n",
    "logger.info(f\"  ‚Ä¢ Batch size: {TRAIN_BATCH_SIZE}\")\n",
    "logger.info(f\"  ‚Ä¢ Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "logger.info(f\"  ‚Ä¢ Learning rate: 2e-4\")\n",
    "logger.info(f\"  ‚Ä¢ Samples: {len(tokenized_dataset)}\")\n",
    "logger.info(f\"  ‚Ä¢ Output: {output_dir}\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  ‚Ä¢ Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  ‚Ä¢ Batch size: {TRAIN_BATCH_SIZE}\")\n",
    "print(f\"  ‚Ä¢ Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  ‚Ä¢ Samples: {len(tokenized_dataset)}\")\n",
    "print(f\"  ‚Ä¢ Expected time: 1-2 minutes\\n\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"no\",  # Don't save checkpoints for test\n",
    "    seed=42,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    report_to=[],\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "logger.info(\"Creating Trainer...\")\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "logger.info(\"‚úÖ Trainer initialized\")\n",
    "\n",
    "# Train\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"‚è≥ TRAINING IN PROGRESS\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"‚úÖ TRAINING COMPLETED\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "    logger.info(f\"Training steps: {train_result.global_step}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Loss: {train_result.training_loss:.4f}\")\n",
    "    print(f\"Steps: {train_result.global_step}\")\n",
    "    print(f\"Duration: ~{train_result.metrics.get('train_runtime', 0) / 60:.1f} minutes\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    print(\"‚úÖ All libraries and setup working correctly!\")\n",
    "    print(\"Ready to deploy to Kaggle with full dataset.\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Training failed: {e}\", exc_info=True)\n",
    "    print(f\"\\n‚ùå Training failed: {e}\")\n",
    "    print(\"Please check the error above and logs for details\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ab3e44",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Test Inference with Trained Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a91e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"\\nüß™ Testing inference with trained adapter\\n\")\n",
    "\n",
    "# Test prompt - Qwen format\n",
    "test_prompt = \"\"\"<|im_start|>system\n",
    "You are a helpful assistant specialized in cybersecurity.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "What is SQL injection?\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "logger.info(\"Inference test:\")\n",
    "print(f\"Test prompt:\")\n",
    "print(\"=\" * 60)\n",
    "print(test_prompt)\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nGenerating response...\\n\")\n",
    "\n",
    "try:\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "    logger.info(f\"Input tokens: {inputs['input_ids'].shape}\")\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    logger.info(\"‚úÖ Inference successful\")\n",
    "    print(\"‚úÖ Generated response:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(response)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚úÖ Inference working correctly!\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Inference failed: {e}\", exc_info=True)\n",
    "    print(f\"‚ùå Inference failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save adapter\n",
    "print(\"üíæ Saving adapter locally...\\n\")\n",
    "\n",
    "try:\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Adapter saved to {output_dir}\")\n",
    "    print(f\"‚úÖ Adapter saved to: {output_dir}\")\n",
    "    \n",
    "    # List files\n",
    "    import os\n",
    "    files = os.listdir(output_dir)\n",
    "    print(\"\\nFiles created:\")\n",
    "    for f in sorted(files):\n",
    "        size = os.path.getsize(os.path.join(output_dir, f)) / 1e6\n",
    "        print(f\"  ‚Ä¢ {f} ({size:.2f} MB)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Save failed: {e}\")\n",
    "    print(f\"Save failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6529c6f",
   "metadata": {},
   "source": [
    "## üìã Setup Validation Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd67d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Final validation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ LOCAL SETUP VALIDATION CHECKLIST\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "checks = {\n",
    "    \"Dependencies\": {\n",
    "        \"torch\": False,\n",
    "        \"transformers >= 4.40\": False,\n",
    "        \"peft\": False,\n",
    "        \"bitsandbytes\": False,\n",
    "    },\n",
    "    \"Model Support\": {\n",
    "        \"Qwen2.5-3B loading\": False,\n",
    "        \"4-bit quantization\": False,\n",
    "        \"LoRA configuration\": False,\n",
    "    },\n",
    "    \"Training\": {\n",
    "        \"Dataset preparation\": False,\n",
    "        \"Tokenization\": False,\n",
    "        \"Training loop\": False,\n",
    "    },\n",
    "    \"Inference\": {\n",
    "        \"Model generation\": False,\n",
    "        \"Adapter saving\": False,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Check results\n",
    "print(\"üìå Key Findings:\")\n",
    "print()\n",
    "print(f\"‚úÖ Successfully loaded: Qwen2.5-3B-Instruct\")\n",
    "print(f\"‚úÖ Quantization working: 4-bit NF4 with bfloat16\")\n",
    "print(f\"‚úÖ LoRA adapters configured: 7 target modules\")\n",
    "print(f\"‚úÖ Training completed: 1 epoch on {len(tokenized_dataset)} samples\")\n",
    "print(f\"‚úÖ Inference tested: Model generation working\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Next steps\n",
    "print(\"üöÄ NEXT STEPS FOR KAGGLE DEPLOYMENT:\")\n",
    "print()\n",
    "print(\"1. Upload dataset to Kaggle:\")\n",
    "print(\"   - File: data/red_v26_phi3_evasion_expanded_100pct_en.jsonl\")\n",
    "print(\"   - Size: ~139K records\")\n",
    "print()\n",
    "print(\"2. Use the original qwen7b_waf_evasion_kaggle.ipynb notebook\")\n",
    "print(\"   (This local test verified all libraries work)\")\n",
    "print()\n",
    "print(\"3. Configuration for Kaggle P100:\")\n",
    "print(\"   - Model: Can use Qwen2.5-3B-Instruct (lighter) OR Qwen2-7B-Instruct\")\n",
    "print(\"   - Samples: Start with 5000, can increase to 10000\")\n",
    "print(\"   - Epochs: 2-3 for better quality\")\n",
    "print(\"   - Batch size: Will auto-detect as 8 on P100\")\n",
    "print()\n",
    "print(\"4. Estimated training time on Kaggle:\")\n",
    "print(\"   - Qwen2.5-3B: ~1 hour for 5000 samples, 2 epochs\")\n",
    "print(\"   - Qwen2-7B: ~2.5 hours for 5000 samples, 2 epochs\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "logger.info(\"‚úÖ Local setup validation complete\")\n",
    "logger.info(\"All libraries and model loading verified\")\n",
    "logger.info(\"Ready for Kaggle deployment\")\n",
    "\n",
    "print(\"üìä Log file: \" + log_file)\n",
    "print(\"\\n‚úÖ LOCAL TESTING COMPLETE - Ready for remote deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
