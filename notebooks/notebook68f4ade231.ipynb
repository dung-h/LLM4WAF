{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Huấn luyện LLM cho WAF Bypass trên Kaggle\n",
    "\n",
    "Notebook này sẽ tự động hóa quá trình tinh chỉnh (fine-tuning) một mô hình ngôn ngữ lớn (LLM) 7 tỷ tham số để tạo payload vượt qua WAF.\n",
    "\n",
    "### **Hướng dẫn Thiết lập (QUAN TRỌNG!)**\n",
    "\n",
    "Trước khi chạy notebook này, bạn **PHẢI** hoàn thành các bước sau:\n",
    "\n",
    "1.  **Tạo Kaggle Dataset:**\n",
    "    *   Trên trang Kaggle, đi đến **Datasets > New Dataset**.\n",
    "    *   Đặt một tiêu đề cho dataset (ví dụ: `my-waf-data`).\n",
    "    *   Tải lên hai tệp của bạn: `red_train_v6_multi_clean.jsonl` và `red_test_v6_multi_clean.jsonl`.\n",
    "    *   Sau khi tạo xong, ghi nhớ tên dataset của bạn (nó sẽ có dạng `username/dataset-name`).\n",
    "\n",
    "2.  **Thêm Dataset vào Notebook:**\n",
    "    *   Trong notebook này, nhấp vào **\"Add Data\"** ở thanh bên phải.\n",
    "    *   Tìm và thêm dataset bạn vừa tạo.\n",
    "\n",
    "3.  **Thêm Hugging Face Token:**\n",
    "    *   Đi tới tab **\"Secrets\"** trong notebook này.\n",
    "    *   Tạo một secret mới có tên là `HF_TOKEN`.\n",
    "    *   Dán token truy cập Hugging Face của bạn làm giá trị của secret.\n",
    "\n",
    "4.  **Chỉnh sửa Ô Cấu hình:**\n",
    "    *   Chạy ô tiếp theo và **chỉnh sửa các giá trị** cho `DATASET_NAME` và `MODEL_TO_TRAIN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T04:16:33.246791Z",
     "iopub.status.busy": "2025-11-13T04:16:33.246469Z",
     "iopub.status.idle": "2025-11-13T04:16:33.251611Z",
     "shell.execute_reply": "2025-11-13T04:16:33.250820Z",
     "shell.execute_reply.started": "2025-11-13T04:16:33.246768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================\n",
    "#  Ô CẤU HÌNH - VUI LÒNG CHỈNH SỬA!  #\n",
    "# ===================================\n",
    "\n",
    "# 1. Đặt tên dataset của bạn ở đây (phần sau /kaggle/input/).\n",
    "# Ví dụ: nếu đường dẫn là /kaggle/input/my-waf-data, hãy đặt là \"my-waf-data\".\n",
    "DATASET_NAME = \"llm4waf\" \n",
    "\n",
    "# 2. Chọn MỘT mô hình để huấn luyện từ danh sách.\n",
    "# Các tùy chọn hợp lệ: \"deepseek\", \"olmoe\", \"openhermes\"\n",
    "MODEL_TO_TRAIN = \"deepseek\"\n",
    "\n",
    "# In ra để xác nhận\n",
    "print(f\"✅ Dataset được đặt thành: /kaggle/input/{DATASET_NAME}\")\n",
    "print(f\"✅ Mô hình được chọn để huấn luyện: {MODEL_TO_TRAIN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bước 1: Cài đặt các Phụ thuộc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T04:16:35.724454Z",
     "iopub.status.busy": "2025-11-13T04:16:35.724139Z",
     "iopub.status.idle": "2025-11-13T04:16:39.950865Z",
     "shell.execute_reply": "2025-11-13T04:16:39.949963Z",
     "shell.execute_reply.started": "2025-11-13T04:16:35.724401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"--- Installing dependencies ---\")\n",
    "%pip install -q --upgrade pip\n",
    "%pip install -q \\\n",
    "    \"torch\" \\\n",
    "    \"transformers>=4.41.0\" \\\n",
    "    \"accelerate>=0.30.0\" \\\n",
    "    \"datasets>=2.19.0\" \\\n",
    "    \"bitsandbytes>=0.43.0\" \\\n",
    "    \"peft>=0.11.1\" \\\n",
    "    \"trl>=0.9.6\" \\\n",
    "    \"pyyaml\"\n",
    "print(\"✅ Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bước 2: Tạo các Tệp Kịch bản và Cấu hình\n",
    "\n",
    "Ô tiếp theo sẽ tạo tất cả các tệp cần thiết trực tiếp trong môi trường notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:01:55.634947Z",
     "iopub.status.busy": "2025-11-13T05:01:55.634232Z",
     "iopub.status.idle": "2025-11-13T05:01:55.642076Z",
     "shell.execute_reply": "2025-11-13T05:01:55.641467Z",
     "shell.execute_reply.started": "2025-11-13T05:01:55.634922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/kaggle_train.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import yaml\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "def require_env_token(var: str = \"HF_TOKEN\") -> None:\n",
    "    '''Check for HF token from Kaggle secrets.'''\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    try:\n",
    "        user_secrets = UserSecretsClient()\n",
    "        token = user_secrets.get_secret(var)\n",
    "        os.environ[var] = token\n",
    "        print(f\"✅ Successfully loaded {var} from Kaggle Secrets.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not load {var} from Kaggle Secrets. Please ensure it is set.\", e)\n",
    "        raise SystemExit(\"Aborting for safety.\")\n",
    "\n",
    "def load_config(path: str) -> Dict[str, Any]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def build_bnb_config(cfg: Dict[str, Any]) -> BitsAndBytesConfig:\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=bool(cfg.get(\"load_in_4bit\", True)),\n",
    "        bnb_4bit_quant_type=cfg.get(\"bnb_4bit_quant_type\", \"nf4\"),\n",
    "        bnb_4bit_use_double_quant=bool(cfg.get(\"bnb_4bit_use_double_quant\", True)),\n",
    "        bnb_4bit_compute_dtype=getattr(torch, cfg.get(\"bnb_4bit_compute_dtype\", \"bfloat16\")),\n",
    "    )\n",
    "\n",
    "def build_lora_config(cfg: Dict[str, Any]) -> LoraConfig:\n",
    "    model_name = cfg.get(\"model_name\", \"\").lower()\n",
    "    if \"mistral\" in model_name or \"hermes\" in model_name:\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "    else:\n",
    "        target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    "\n",
    "    return LoraConfig(\n",
    "        r=int(cfg.get(\"lora_r\", 16)),\n",
    "        lora_alpha=int(cfg.get(\"lora_alpha\", 32)),\n",
    "        target_modules=cfg.get(\"target_modules\", target_modules),\n",
    "        lora_dropout=float(cfg.get(\"lora_dropout\", 0.05)),\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "def format_example(example: Dict[str, Any], fields: Dict[str, str], prompt_format_type: str = \"default\") -> str:\n",
    "    payload = (example.get(fields[\"payload\"]) or \"\").strip()\n",
    "    reasoning = (example.get(fields[\"reasoning\"]) or \"\").strip()\n",
    "    instr = (example.get(fields.get(\"instruction\")) or \"\").strip()\n",
    "    ctx = (example.get(fields.get(\"context\")) or \"\").strip()\n",
    "    cons = (example.get(fields.get(\"constraints\")) or \"\").strip()\n",
    "\n",
    "    # Tuyệt đối không xuống dòng trong string literal\n",
    "    user_message = (\n",
    "        \"Instruction: \" + instr + \"\\n\"\n",
    "        + \"Context: \" + ctx + \"\\n\"\n",
    "        + \"Constraints: \" + cons\n",
    "    )\n",
    "\n",
    "    assistant_message = (\n",
    "        \"Payload: \" + payload + \"\\n\"\n",
    "        + \"Reasoning: \" + reasoning\n",
    "    )\n",
    "\n",
    "    if prompt_format_type == \"openhermes\":\n",
    "        return (\n",
    "            \"<|im_start|>user\\n\"\n",
    "            + user_message\n",
    "            + \"<|im_end|>\\n\"\n",
    "            + \"<|im_start|>assistant\\n\"\n",
    "            + assistant_message\n",
    "            + \"<|im_end|>\"\n",
    "        )\n",
    "    else:\n",
    "        return user_message + \"\\n\\n\" + assistant_message\n",
    "\n",
    "def formatting_func(examples: Dict[str, List[str]], fields: Dict[str, str] | None, prompt_format_type: str) -> List[str]:\n",
    "    # Nếu không truyền mapping từ config, dùng default\n",
    "    if fields is None:\n",
    "        fields = {\n",
    "            \"instruction\": \"instruction\",\n",
    "            \"context\": \"context\",\n",
    "            \"constraints\": \"constraints\",\n",
    "            \"payload\": \"payload\",\n",
    "            \"reasoning\": \"reasoning\",\n",
    "        }\n",
    "\n",
    "    out = []\n",
    "    payload_col = fields[\"payload\"]  # tên cột thực trong dataset (thường là \"payload\")\n",
    "\n",
    "    for i in range(len(examples[payload_col])):\n",
    "        ex = {k: examples[k][i] for k in examples}\n",
    "        out.append(format_example(ex, fields, prompt_format_type))\n",
    "    return out\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config\", required=True, help=\"Path to YAML config\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    require_env_token(\"HF_TOKEN\")\n",
    "\n",
    "    cfg = load_config(args.config)\n",
    "    model_name = cfg[\"model_name\"]\n",
    "    auth_token = os.environ.get(\"HF_TOKEN\")\n",
    "    prompt_format_type = cfg.get(\"prompt_format_type\", \"default\")\n",
    "\n",
    "    bnb_cfg = build_bnb_config(cfg)\n",
    "\n",
    "    print(f\"Loading tokenizer for {model_name}...\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, token=auth_token, trust_remote_code=True)\n",
    "    tok.padding_side = cfg.get(\"padding_side\", \"left\")\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    print(f\"Loading 4-bit model: {model_name} (this may take a while)...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=auth_token,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    if cfg.get(\"gradient_checkpointing\", True):\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.config.use_cache = False\n",
    "\n",
    "    print(\"Preparing k-bit training and applying LoRA adapters…\")\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    lora_cfg = build_lora_config(cfg)\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "    data_files = {\"train\": cfg[\"train_path\"]}\n",
    "    if cfg.get(\"eval_path\") and os.path.exists(cfg[\"eval_path\"]):\n",
    "        data_files[\"validation\"] = cfg[\"eval_path\"]\n",
    "\n",
    "    ds = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "    text_fields = cfg.get(\"text_fields\", None)\n",
    "\n",
    "    def _fmt_func(batch):\n",
    "        return {\"text\": formatting_func(batch, text_fields, prompt_format_type)}\n",
    "\n",
    "    ds_proc = ds[\"train\"].map(_fmt_func, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "\n",
    "    out_dir = cfg.get(\"output_dir\")\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=out_dir,\n",
    "        per_device_train_batch_size=int(cfg.get(\"per_device_train_batch_size\", 1)),\n",
    "        gradient_accumulation_steps=int(cfg.get(\"gradient_accumulation_steps\", 8)),\n",
    "        num_train_epochs=float(cfg.get(\"num_train_epochs\", 3)),\n",
    "        learning_rate=float(cfg.get(\"learning_rate\", 2e-4)),\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        logging_steps=int(cfg.get(\"logging_steps\", 10)),\n",
    "        save_strategy=\"epoch\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        report_to=[],\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=sft_config,\n",
    "        train_dataset=ds_proc,\n",
    "        processing_class=tok,  # bạn đã đổi từ tokenizer -> processing_class trước đó\n",
    "    )\n",
    "\n",
    "    print(\"Starting training…\")\n",
    "    trainer.train()\n",
    "    \n",
    "    final_adapter_path = os.path.join(out_dir, \"adapter\")\n",
    "    print(f\"Saving final adapter to {final_adapter_path}...\")\n",
    "    trainer.model.save_pretrained(final_adapter_path)\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T04:51:33.978358Z",
     "iopub.status.busy": "2025-11-13T04:51:33.977779Z",
     "iopub.status.idle": "2025-11-13T04:51:33.985913Z",
     "shell.execute_reply": "2025-11-13T04:51:33.985111Z",
     "shell.execute_reply.started": "2025-11-13T04:51:33.978333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Tạo các thư mục cần thiết\n",
    "os.makedirs(\"scripts\", exist_ok=True)\n",
    "os.makedirs(\"configs\", exist_ok=True)\n",
    "\n",
    "\n",
    "# with open(\"scripts/kaggle_train.py\", \"w\") as f:\n",
    "#     f.write(kaggle_train_script)\n",
    "\n",
    "# print(\"Created scripts/kaggle_train.py\")\n",
    "\n",
    "# Create config files\n",
    "def create_config(file_path, content):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "deepseek_config = f\"\"\"\n",
    "model_name: \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "prompt_format_type: \"default\"\n",
    "train_path: \"/kaggle/input/{DATASET_NAME}/red_train_v6_multi_clean.jsonl\"\n",
    "lora_r: 16\n",
    "num_train_epochs: 3\n",
    "output_dir: \"/kaggle/working/deepseek_7b_base_adapter\"\n",
    "\"\"\"\n",
    "create_config(\"configs/kaggle_deepseek_7b_base.yaml\", deepseek_config)\n",
    "print(\"Created configs/kaggle_deepseek_7b_base.yaml\")\n",
    "\n",
    "olmoe_config = f\"\"\"\n",
    "model_name: \"allenai/OLMoE-1B-7B-0125\"\n",
    "prompt_format_type: \"default\"\n",
    "train_path: \"/kaggle/input/{DATASET_NAME}/red_train_v6_multi_clean.jsonl\"\n",
    "lora_r: 16\n",
    "num_train_epochs: 3\n",
    "output_dir: \"/kaggle/working/olmoe_7b_adapter\"\n",
    "\"\"\"\n",
    "create_config(\"configs/kaggle_olmoe_7b.yaml\", olmoe_config)\n",
    "print(\"Created configs/kaggle_olmoe_7b.yaml\")\n",
    "\n",
    "openhermes_config = f\"\"\"\n",
    "model_name: \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "prompt_format_type: \"openhermes\"\n",
    "train_path: \"/kaggle/input/{DATASET_NAME}/red_train_v6_multi_clean.jsonl\"\n",
    "lora_r: 16\n",
    "num_train_epochs: 3\n",
    "output_dir: \"/kaggle/working/openhermes_7b_adapter\"\n",
    "\"\"\"\n",
    "create_config(\"configs/kaggle_openhermes_7b.yaml\", openhermes_config)\n",
    "print(\"Created configs/kaggle_openhermes_7b.yaml\")\n",
    "\n",
    "print(\"\\n✅ All files created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bước 3: Chạy Huấn luyện\n",
    "\n",
    "Ô tiếp theo sẽ chạy tập lệnh huấn luyện cho mô hình bạn đã chọn trong ô cấu hình đầu tiên. Quá trình này sẽ mất nhiều thời gian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:02:01.878462Z",
     "iopub.status.busy": "2025-11-13T05:02:01.877882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Đọc lại các biến từ ô cấu hình (để đảm bảo chúng được đặt)\n",
    "config_map = {\n",
    "    \"deepseek\": \"configs/kaggle_deepseek_7b_base.yaml\",\n",
    "    \"olmoe\": \"configs/kaggle_olmoe_7b.yaml\",\n",
    "    \"openhermes\": \"configs/kaggle_openhermes_7b.yaml\"\n",
    "}\n",
    "\n",
    "config_file = config_map.get(MODEL_TO_TRAIN)\n",
    "\n",
    "if config_file:\n",
    "    print(f\"--- Starting training for {MODEL_TO_TRAIN} ---\")\n",
    "    # Sử dụng os.system để chạy lệnh shell\n",
    "    exit_code = os.system(f\"python scripts/kaggle_train.py --config {config_file}\")\n",
    "    if exit_code != 0:\n",
    "        raise RuntimeError(f\"Training failed with exit code {exit_code}\")\n",
    "else:\n",
    "    print(f\"ERROR: Invalid model choice '{MODEL_TO_TRAIN}'. Please choose from {list(config_map.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bước 4: Đóng gói và Tải xuống Kết quả\n",
    "\n",
    "Sau khi huấn luyện hoàn tất, chạy ô sau để nén bộ điều hợp (adapter) đã huấn luyện thành một tệp `.tar.gz`. Sau đó, bạn có thể tìm thấy tệp này trong phần **\"Output\"** ở thanh bên phải và tải nó xuống."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "adapter_dir_map = {\n",
    "    \"deepseek\": \"deepseek_7b_base_adapter\",\n",
    "    \"olmoe\": \"olmoe_7b_adapter\",\n",
    "    \"openhermes\": \"openhermes_7b_adapter\"\n",
    "}\n",
    "\n",
    "adapter_folder_name = adapter_dir_map.get(MODEL_TO_TRAIN)\n",
    "output_path = f\"/kaggle/working/{adapter_folder_name}\"\n",
    "archive_name = f\"{adapter_folder_name}.tar.gz\"\n",
    "\n",
    "if adapter_folder_name and os.path.exists(output_path):\n",
    "    print(f\"--- Packaging adapter from {output_path} ---\")\n",
    "    # Lệnh tar: -c: tạo archive, -z: nén với gzip, -f: chỉ định tên tệp, -C: thay đổi thư mục trước khi nén\n",
    "    os.system(f\"tar -czf {archive_name} -C {output_path} .\")\n",
    "    print(f\"✅ Adapter packaged to /kaggle/working/{archive_name}\")\n",
    "    print(\"You can now find this file in the 'Output' section and download it.\")\n",
    "else:\n",
    "    print(f\"ERROR: Could not find adapter directory '{output_path}' to package. Did training complete successfully?\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8723734,
     "sourceId": 13712849,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 270740729,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 276991546,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
