{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53140002",
   "metadata": {},
   "source": [
    "# Qwen-7B LoRA Fine-tuning for WAF Evasion (Kaggle)\n",
    "\n",
    "**Setup**: Train Qwen-7B on 5000 WAF evasion samples using LoRA on Kaggle GPU.\n",
    "\n",
    "**What you need to do**:\n",
    "1. Upload `red_v26_phi3_evasion_expanded_100pct_en.jsonl` to Kaggle input directory\n",
    "2. Run all cells sequentially\n",
    "3. Download the adapter zip from output directory\n",
    "\n",
    "**Execution time**: ~2-3 hours for 5000 samples on P100 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd1ca82",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Required Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dc1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# üìä SETUP LOGGING\n",
    "log_file = f\"/kaggle/working/training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"üöÄ Starting Qwen-7B LoRA fine-tuning pipeline\")\n",
    "\n",
    "# üîê HuggingFace Token Setup (Required for Qwen model access)\n",
    "logger.info(\"üîë Checking HuggingFace token...\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", None)  # Try from env first\n",
    "if not HF_TOKEN:\n",
    "    print(\"‚ö†Ô∏è HF_TOKEN not found in environment!\")\n",
    "    print(\"Option 1: Set via environment variable: export HF_TOKEN=your_token\")\n",
    "    print(\"Option 2: Run this cell with token:\")\n",
    "    print(\"   os.environ['HF_TOKEN'] = 'your_hf_token_here'\\n\")\n",
    "    HF_TOKEN = input(\"Enter your HuggingFace token (or press Enter to continue): \").strip()\n",
    "    if HF_TOKEN:\n",
    "        os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "if HF_TOKEN:\n",
    "    logger.info(f\"‚úÖ HuggingFace token set (length: {len(HF_TOKEN)} chars)\")\n",
    "    print(f\"‚úÖ HuggingFace token set\")\n",
    "else:\n",
    "    logger.warning(\"‚ö†Ô∏è No HF token provided. Model loading may fail.\")\n",
    "    print(\"‚ö†Ô∏è No HF token provided. Model loading may fail if authentication needed.\\n\")\n",
    "\n",
    "# ‚ö†Ô∏è CRITICAL FIX: Force reinstall transformers to latest with all dependencies\n",
    "logger.info(\"üì¶ Updating transformers to latest version...\")\n",
    "print(\"üì¶ Upgrading transformers (critical for Qwen2Tokenizer)...\")\n",
    "\n",
    "try:\n",
    "    # Force reinstall transformers with all dependencies including safetensors\n",
    "    subprocess.check_call([\n",
    "        sys.executable, \"-m\", \"pip\", \n",
    "        \"install\", \"--upgrade\", \"--force-reinstall\", \"-q\",\n",
    "        \"transformers\",\n",
    "        \"safetensors\",\n",
    "        \"huggingface-hub\"\n",
    "    ])\n",
    "    logger.info(\"‚úÖ Transformers upgraded to latest\")\n",
    "    print(\"‚úÖ Transformers upgraded to latest\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"‚ö†Ô∏è Forced update failed: {e}, trying standard update...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"-q\", \"transformers\"])\n",
    "\n",
    "# Install other required packages\n",
    "packages = [\n",
    "    \"torch==2.1.2\",              # ‚úì Compatible with bitsandbytes, transformers\n",
    "    \"peft==0.7.1\",               # ‚úì LoRA support\n",
    "    \"bitsandbytes==0.41.3.post2\",# ‚úì 4-bit quantization\n",
    "    \"datasets==2.14.5\",          # ‚úì Dataset handling\n",
    "    \"accelerate==0.24.1\",        # ‚úì Multi-GPU support\n",
    "]\n",
    "\n",
    "logger.info(f\"üì¶ Installing {len(packages)} dependencies...\")\n",
    "print(f\"üì¶ Installing other dependencies...\")\n",
    "\n",
    "for i, package in enumerate(packages, 1):\n",
    "    try:\n",
    "        logger.info(f\"  [{i}/{len(packages)}] Installing {package}...\")\n",
    "        print(f\"  ‚Üí {package}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "        logger.info(f\"  ‚úì {package} installed\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to install {package}: {e}\")\n",
    "        raise\n",
    "\n",
    "logger.info(\"‚úÖ All dependencies installed successfully!\")\n",
    "print(\"\\n‚úÖ All dependencies installed successfully!\")\n",
    "print(f\"üìä Logs saved to: {log_file}\")\n",
    "\n",
    "# Verify transformers version\n",
    "import transformers\n",
    "logger.info(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"\\nüìã Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Verify Qwen2 support\n",
    "try:\n",
    "    from transformers.models.qwen2 import Qwen2Tokenizer\n",
    "    logger.info(\"‚úÖ Qwen2Tokenizer is available\")\n",
    "    print(\"‚úÖ Qwen2 support verified\")\n",
    "except ImportError:\n",
    "    logger.warning(\"‚ö†Ô∏è Qwen2Tokenizer not found - will use fallback model (Qwen-7B-Chat)\")\n",
    "    print(\"‚ö†Ô∏è Will use Qwen-7B-Chat (stable) instead of Qwen2-7B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6469bfd",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Download and Load Qwen-7B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df7364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "import logging\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"üîß Loading Qwen-7B model with API compatibility checks...\")\n",
    "print(\"üîß Loading Qwen-7B model...\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.info(f\"Device: {device}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Detect GPU VRAM and auto-select batch size\n",
    "if device == \"cuda\":\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    logger.info(f\"GPU: {gpu_name} | VRAM: {vram_gb:.2f} GB\")\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {vram_gb:.2f} GB\")\n",
    "    \n",
    "    # Auto-detect optimal batch size based on VRAM\n",
    "    if vram_gb >= 40:\n",
    "        AUTO_BATCH_SIZE = 32\n",
    "    elif vram_gb >= 24:\n",
    "        AUTO_BATCH_SIZE = 16\n",
    "    elif vram_gb >= 16:\n",
    "        AUTO_BATCH_SIZE = 8\n",
    "    else:\n",
    "        AUTO_BATCH_SIZE = 4\n",
    "    \n",
    "    logger.info(f\"Auto-detected batch size: {AUTO_BATCH_SIZE}\")\n",
    "    print(f\"Auto-detected batch size: {AUTO_BATCH_SIZE}\\n\")\n",
    "else:\n",
    "    logger.warning(\"‚ö†Ô∏è CPU mode detected - training will be VERY slow!\")\n",
    "    print(\"‚ö†Ô∏è CPU mode detected - training will be VERY slow!\")\n",
    "    AUTO_BATCH_SIZE = 2\n",
    "\n",
    "# 4-bit quantization config (compatible with Qwen-7B)\n",
    "logger.info(\"‚öôÔ∏è Configuring 4-bit quantization (nf4)...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "logger.info(\"‚úì BitsAndBytes config set\")\n",
    "\n",
    "# Get HF token\n",
    "hf_token = os.getenv(\"HF_TOKEN\", None)\n",
    "\n",
    "# ‚ö†Ô∏è HANDLE Qwen2Tokenizer REGISTRY ISSUE\n",
    "logger.info(\"üìå Fixing Qwen2Tokenizer registry issue...\")\n",
    "try:\n",
    "    # This will register Qwen2Tokenizer in the tokenizer registry\n",
    "    from transformers.models.qwen2 import Qwen2Tokenizer, Qwen2TokenizerFast\n",
    "    logger.info(\"‚úì Qwen2 tokenizers registered\")\n",
    "except ImportError as e:\n",
    "    logger.warning(f\"‚ö†Ô∏è Qwen2 import failed: {e}\")\n",
    "    logger.info(\"Re-registering Qwen2 modules...\")\n",
    "    try:\n",
    "        subprocess.check_call([\n",
    "            sys.executable, \"-c\",\n",
    "            \"from transformers.models.qwen2 import Qwen2Tokenizer; print('Qwen2Tokenizer registered')\"\n",
    "        ])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Model selection with fallback\n",
    "logger.info(\"üì• Loading tokenizer...\")\n",
    "print(\"üì• Loading tokenizer...\")\n",
    "\n",
    "model_id = \"Qwen/Qwen2-7B-Instruct\"\n",
    "hf_token = os.getenv(\"HF_TOKEN\", None)\n",
    "\n",
    "try:\n",
    "    # Try Qwen2-7B-Instruct first\n",
    "    logger.info(f\"Attempting to load: {model_id}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=True,\n",
    "        token=hf_token,\n",
    "    )\n",
    "    logger.info(f\"‚úÖ Tokenizer loaded: Qwen2-7B-Instruct\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    # Fallback to Qwen-7B-Chat (original, more stable)\n",
    "    logger.warning(f\"‚ö†Ô∏è Qwen2-7B failed: {str(e)[:80]}\")\n",
    "    logger.info(\"üìå Falling back to Qwen-7B-Chat (stable version)...\")\n",
    "    print(\"‚ö†Ô∏è Falling back to Qwen-7B-Chat...\")\n",
    "    \n",
    "    model_id = \"Qwen/Qwen-7B-Chat\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=True,\n",
    "            token=hf_token,\n",
    "        )\n",
    "        logger.info(f\"‚úÖ Tokenizer loaded: Qwen-7B-Chat\")\n",
    "    except Exception as e2:\n",
    "        logger.error(f\"‚ùå Both models failed: {e2}\")\n",
    "        raise\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "logger.info(f\"‚úÖ Tokenizer ready: {tokenizer.__class__.__name__} | Vocab: {tokenizer.vocab_size}\")\n",
    "print(f\"‚úÖ Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "# Load model with quantization\n",
    "logger.info(f\"üì• Loading model {model_id} with 4-bit quantization...\")\n",
    "print(\"üì• Loading model (this may take 2-3 minutes)...\")\n",
    "\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        token=hf_token,\n",
    "    )\n",
    "    logger.info(f\"‚úÖ Model loaded with flash_attention_2\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.warning(f\"‚ö†Ô∏è Flash attention 2 not available: {str(e)[:80]}\")\n",
    "    logger.info(\"Falling back to default attention...\")\n",
    "    print(\"‚ö†Ô∏è Using default attention...\")\n",
    "    \n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            token=hf_token,\n",
    "        )\n",
    "        logger.info(f\"‚úÖ Model loaded with default attention\")\n",
    "    except Exception as e2:\n",
    "        logger.error(f\"‚ùå Model loading failed: {e2}\")\n",
    "        raise\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "logger.info(f\"‚úÖ Model loaded successfully\")\n",
    "logger.info(f\"  ‚Ä¢ Model: {model_id}\")\n",
    "logger.info(f\"  ‚Ä¢ Parameters: {total_params / 1e9:.2f}B\")\n",
    "logger.info(f\"  ‚Ä¢ Dtype: {model.dtype}\")\n",
    "logger.info(f\"  ‚Ä¢ Device: {next(model.parameters()).device}\")\n",
    "logger.info(f\"  ‚Ä¢ Quantization: 4-bit NF4\")\n",
    "print(f\"‚úÖ Model loaded: {model_id}\")\n",
    "print(f\"Model parameters: {total_params / 1e9:.2f}B\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "print(f\"Auto batch size: {AUTO_BATCH_SIZE} (can override later)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9085f284",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"üìÇ Scanning for WAF dataset...\")\n",
    "print(\"üìÇ Scanning for WAF dataset...\\n\")\n",
    "\n",
    "# Default dataset path in Kaggle\n",
    "DEFAULT_DATASET = \"red_v26_phi3_evasion_expanded_100pct_en.jsonl\"\n",
    "input_dir = Path(\"/kaggle/input/waf-dataset\")\n",
    "\n",
    "if not input_dir.exists():\n",
    "    logger.warning(f\"Directory not found: {input_dir}\")\n",
    "    print(f\"‚ö†Ô∏è Directory not found: {input_dir}\")\n",
    "    print(\"Searching in /kaggle/input/ instead...\\n\")\n",
    "    input_dir = Path(\"/kaggle/input\")\n",
    "    logger.info(f\"Searching in {input_dir}\")\n",
    "\n",
    "# First, look for the default dataset\n",
    "dataset_path = None\n",
    "for path in input_dir.rglob(DEFAULT_DATASET):\n",
    "    dataset_path = path\n",
    "    logger.info(f\"‚úÖ Found default dataset: {path.name}\")\n",
    "    print(f\"‚úÖ Found default dataset: {path.name}\\n\")\n",
    "    break\n",
    "\n",
    "# If not found, list all available JSONL files\n",
    "if dataset_path is None:\n",
    "    logger.warning(f\"Default dataset '{DEFAULT_DATASET}' not found\")\n",
    "    jsonl_files = sorted(list(input_dir.rglob(\"*.jsonl\")))\n",
    "    \n",
    "    if not jsonl_files:\n",
    "        logger.error(\"‚ùå No .jsonl dataset found in /kaggle/input/waf-dataset/\")\n",
    "        raise FileNotFoundError(\"‚ùå No .jsonl dataset found in /kaggle/input/waf-dataset/\")\n",
    "    \n",
    "    logger.info(f\"Found {len(jsonl_files)} JSONL files\")\n",
    "    print(f\"‚ö†Ô∏è Default dataset '{DEFAULT_DATASET}' not found\")\n",
    "    print(f\"Available JSONL files ({len(jsonl_files)}):\\n\")\n",
    "    \n",
    "    file_info = []\n",
    "    for idx, path in enumerate(jsonl_files, 1):\n",
    "        # Count records\n",
    "        record_count = 0\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for _ in f:\n",
    "                record_count += 1\n",
    "        \n",
    "        file_size_mb = path.stat().st_size / 1e6\n",
    "        file_info.append({\n",
    "            'idx': idx,\n",
    "            'path': path,\n",
    "            'records': record_count,\n",
    "            'size_mb': file_size_mb\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"  [{idx}] {path.name}: {record_count:,} records, {file_size_mb:.2f} MB\")\n",
    "        print(f\"[{idx}] {path.name} ({record_count:,} records, {file_size_mb:.2f} MB)\")\n",
    "    \n",
    "    # Auto-select largest file by record count\n",
    "    selected_idx = max(range(len(file_info)), key=lambda i: file_info[i]['records'])\n",
    "    dataset_path = file_info[selected_idx]['path']\n",
    "    logger.info(f\"‚úÖ Auto-selected: {dataset_path.name}\")\n",
    "    print(f\"\\n‚úÖ Selected: {dataset_path.name}\")\n",
    "\n",
    "total_records_available = 0\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    for _ in f:\n",
    "        total_records_available += 1\n",
    "\n",
    "logger.info(f\"üì• Loading dataset: {dataset_path.name} ({total_records_available:,} records)\")\n",
    "print(f\"üì• Loading dataset: {dataset_path.name}\")\n",
    "print(f\"Total records available: {total_records_available:,}\\n\")\n",
    "\n",
    "# Load JSONL file\n",
    "records = []\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            records.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "logger.info(f\"‚úÖ Loaded {len(records):,} valid records\")\n",
    "print(f\"‚úÖ Loaded {len(records):,} valid records\")\n",
    "\n",
    "# ‚öôÔ∏è SAMPLE SIZE CONFIGURATION\n",
    "SAMPLE_SIZE = 5000  # ‚Üê EDIT THIS VALUE\n",
    "\n",
    "logger.info(f\"‚è±Ô∏è Training time estimates on Qwen-7B, 2 epochs, P100 GPU:\")\n",
    "print(f\"\\n‚è±Ô∏è Training time estimate (Qwen-7B, 2 epochs on P100):\")\n",
    "print(f\"   1,000 samples  ‚Üí ~20 min\")\n",
    "print(f\"   5,000 samples  ‚Üí ~1.5 hours (DEFAULT)\")\n",
    "print(f\"   10,000 samples ‚Üí ~3 hours\")\n",
    "print(f\"   20,000 samples ‚Üí ~6 hours\")\n",
    "print(f\"   50,000 samples ‚Üí ~15 hours\\n\")\n",
    "\n",
    "if len(records) > SAMPLE_SIZE:\n",
    "    logger.info(f\"üé≤ Sampling {SAMPLE_SIZE:,} from {len(records):,} records...\")\n",
    "    print(f\"üé≤ Sampling {SAMPLE_SIZE:,} from {len(records):,} records...\")\n",
    "    records = random.sample(records, SAMPLE_SIZE)\n",
    "    logger.info(f\"‚úÖ Using {len(records):,} samples for training\")\n",
    "    print(f\"‚úÖ Using {len(records):,} samples for training\")\n",
    "    print(f\"‚è∞ Estimated training time: {1.5 * (SAMPLE_SIZE/5000):.1f} hours\")\n",
    "    logger.info(f\"‚è∞ Estimated training time: {1.5 * (SAMPLE_SIZE/5000):.1f} hours\")\n",
    "else:\n",
    "    logger.warning(f\"Only {len(records):,} records available (less than {SAMPLE_SIZE:,})\")\n",
    "    print(f\"‚ö†Ô∏è Only {len(records):,} records available (less than {SAMPLE_SIZE:,})\")\n",
    "    print(f\"‚úÖ Using all {len(records):,} samples\")\n",
    "    print(f\"‚è∞ Estimated training time: {1.5 * (len(records)/5000):.1f} hours\")\n",
    "    logger.info(f\"‚è∞ Using all {len(records):,} samples | Est. time: {1.5 * (len(records)/5000):.1f} hours\")\n",
    "\n",
    "# Check record structure\n",
    "logger.info(f\"Record structure: {list(records[0].keys())}\")\n",
    "print(f\"\\nüìã Record structure:\")\n",
    "print(f\"   Keys: {list(records[0].keys())}\")\n",
    "print(f\"\\nüìù Sample record (first 500 chars):\")\n",
    "sample_text = json.dumps(records[0], indent=2)[:500]\n",
    "print(sample_text + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def convert_to_qwen_format(record):\n",
    "    \"\"\"\n",
    "    Convert to Qwen-7B exact chat format\n",
    "    IMPORTANT: Qwen requires exact format with newlines\n",
    "    \n",
    "    Format:\n",
    "    <|im_start|>system\n",
    "    {system_prompt}\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    {user_message}\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    {assistant_response}\n",
    "    <|im_end|>\n",
    "    \"\"\"\n",
    "    if \"messages\" not in record:\n",
    "        return \"\"\n",
    "    \n",
    "    messages = record[\"messages\"]\n",
    "    if not messages or len(messages) < 2:\n",
    "        return \"\"\n",
    "    \n",
    "    # Build with exact Qwen format (system + user + assistant)\n",
    "    text = \"<|im_start|>system\\nYou are a helpful cybersecurity assistant specialized in WAF evasion techniques and SQL injection/XSS payload generation.\\n<|im_end|>\\n\"\n",
    "    \n",
    "    for msg in messages:\n",
    "        role = msg.get(\"role\", \"\").lower()\n",
    "        content = msg.get(\"content\", \"\").strip()\n",
    "        \n",
    "        if not content:\n",
    "            continue\n",
    "        \n",
    "        # Ensure role is valid\n",
    "        if role not in [\"user\", \"assistant\"]:\n",
    "            continue\n",
    "        \n",
    "        # Build message block\n",
    "        text += f\"<|im_start|>{role}\\n{content}\\n<|im_end|>\\n\"\n",
    "    \n",
    "    # Must end with assistant token for training\n",
    "    if not text.rstrip().endswith(\"<|im_end|>\"):\n",
    "        text += \"<|im_start|>assistant\\n<|im_end|>\"\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Preprocess data\n",
    "logger.info(\"üîÑ Converting dataset to Qwen-7B format...\")\n",
    "print(\"üîÑ Converting dataset to Qwen format...\")\n",
    "processed_data = []\n",
    "skipped = 0\n",
    "\n",
    "for i, record in enumerate(records):\n",
    "    text = convert_to_qwen_format(record)\n",
    "    if text:\n",
    "        processed_data.append({\"text\": text})\n",
    "    else:\n",
    "        skipped += 1\n",
    "\n",
    "logger.info(f\"‚úÖ Converted {len(processed_data)} records, skipped {skipped}\")\n",
    "print(f\"‚úÖ Converted {len(processed_data)} records\")\n",
    "if skipped > 0:\n",
    "    logger.warning(f\"‚ö†Ô∏è Skipped {skipped} invalid records\")\n",
    "    print(f\"‚ö†Ô∏è Skipped {skipped} invalid records\")\n",
    "\n",
    "logger.info(f\"üìã Sample formatted text (first 700 chars)\")\n",
    "print(f\"\\nüìã Sample formatted text (Qwen format):\")\n",
    "sample = processed_data[0]['text']\n",
    "print(\"=\" * 60)\n",
    "print(sample[:700])\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create Hugging Face dataset\n",
    "dataset = Dataset.from_dict({\"text\": [r[\"text\"] for r in processed_data]})\n",
    "avg_words = sum(len(r['text'].split()) for r in processed_data) // len(processed_data)\n",
    "logger.info(f\"‚úÖ Dataset created: {len(dataset)} samples | Avg {avg_words} words/sample\")\n",
    "print(f\"\\n‚úÖ Dataset created: {len(dataset)} samples\")\n",
    "print(f\"Avg tokens per sample: ~{avg_words} words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8879fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize and prepare data for training\"\"\"\n",
    "    max_seq_length = 1024  # Increased for better Qwen-7B context utilization\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    \n",
    "    # Set labels = input_ids for causal language modeling\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "logger.info(\"‚öôÔ∏è Tokenizing dataset...\")\n",
    "print(\"‚öôÔ∏è Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    remove_columns=[\"text\"],\n",
    "    desc=\"Tokenizing\",\n",
    ")\n",
    "\n",
    "logger.info(f\"‚úÖ Tokenization complete | Samples: {len(tokenized_dataset)} | Max seq: 1024\")\n",
    "print(f\"‚úÖ Tokenization complete\")\n",
    "print(f\"Sample token ids length: {len(tokenized_dataset[0]['input_ids'])}\")\n",
    "print(f\"Dataset ready: {len(tokenized_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5f93b",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Configure LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42935c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"‚öôÔ∏è Setting up adapter method...\")\n",
    "print(\"‚öôÔ∏è Setting up adapter method...\\n\")\n",
    "\n",
    "# ‚ö° CHOOSE YOUR METHOD:\n",
    "USE_UNSLOTH = False  # ‚Üê CHANGE TO True FOR FASTER TRAINING\n",
    "\n",
    "if USE_UNSLOTH:\n",
    "    try:\n",
    "        logger.info(\"‚ö° Attempting to use UNSLOTH...\")\n",
    "        from unsloth import FastLanguageModel\n",
    "        \n",
    "        logger.info(\"‚ö° Using UNSLOTH for fast training (2-3x speedup)\")\n",
    "        print(\"‚ö° Using UNSLOTH for fast training...\\n\")\n",
    "        print(\"Expected speedup: 2-3x faster\")\n",
    "        print(f\"Expected time: ~{int(1.5 * (SAMPLE_SIZE/5000) / 2.5)} minutes for {SAMPLE_SIZE:,} samples\\n\")\n",
    "        \n",
    "        # Convert model for fast training\n",
    "        model, tokenizer = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.05,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=True,\n",
    "            use_rslora=False,\n",
    "        )\n",
    "        logger.info(\"‚úÖ Unsloth adapter configured\")\n",
    "        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        logger.info(f\"Trainable: {trainable / 1e6:.2f}M params\")\n",
    "        print(\"‚úÖ Unsloth adapter configured\")\n",
    "        print(f\"Trainable parameters: {trainable / 1e6:.2f}M\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        logger.warning(f\"‚ö†Ô∏è Unsloth not available: {e}\")\n",
    "        logger.warning(\"Falling back to PEFT...\")\n",
    "        print(\"‚ö†Ô∏è Unsloth not installed, falling back to PEFT...\")\n",
    "        USE_UNSLOTH = False\n",
    "\n",
    "if not USE_UNSLOTH:\n",
    "    logger.info(\"üìå Using standard PEFT LoRA (compatible, slightly slower)\")\n",
    "    print(\"üìå Using standard PEFT\\n\")\n",
    "    \n",
    "    # LoRA config - ALL projection layers for Qwen-7B\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\n",
    "            \"q_proj\",      # Query projection (Attention)\n",
    "            \"v_proj\",      # Value projection (Attention)\n",
    "            \"k_proj\",      # Key projection (Attention)\n",
    "            \"o_proj\",      # Output projection (Attention)\n",
    "            \"gate_proj\",   # Gate projection (MLP)\n",
    "            \"up_proj\",     # Up projection (MLP)\n",
    "            \"down_proj\",   # Down projection (MLP)\n",
    "        ],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "    )\n",
    "    \n",
    "    # Verify target modules exist in Qwen-7B\n",
    "    logger.info(\"Verifying LoRA target modules for Qwen-7B...\")\n",
    "    model_params = set()\n",
    "    for name, _ in model.named_parameters():\n",
    "        # Extract base name (e.g., \"q_proj\" from \"lm_head.q_proj\" or \"model.layers.0.self_attn.q_proj\")\n",
    "        for target in lora_config.target_modules:\n",
    "            if target in name:\n",
    "                model_params.add(target)\n",
    "    \n",
    "    logger.info(f\"‚úì Verified target modules in model: {sorted(model_params)}\")\n",
    "    \n",
    "    # Wrap model with LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    logger.info(\"‚úÖ LoRA adapter configured for Qwen-7B\")\n",
    "    logger.info(f\"  ‚Ä¢ LoRA Rank: {lora_config.r}\")\n",
    "    logger.info(f\"  ‚Ä¢ Alpha: {lora_config.lora_alpha}\")\n",
    "    logger.info(f\"  ‚Ä¢ Dropout: {lora_config.lora_dropout}\")\n",
    "    logger.info(f\"  ‚Ä¢ Target modules: {len(lora_config.target_modules)} (Attention: 4, MLP: 3)\")\n",
    "    logger.info(f\"  ‚Ä¢ Task type: CAUSAL_LM\")\n",
    "    \n",
    "    print(f\"LoRA Configuration (Qwen-7B optimized):\")\n",
    "    print(f\"  ‚Ä¢ Rank (r): {lora_config.r}\")\n",
    "    print(f\"  ‚Ä¢ Alpha: {lora_config.lora_alpha}\")\n",
    "    print(f\"  ‚Ä¢ Dropout: {lora_config.lora_dropout}\")\n",
    "    print(f\"  ‚Ä¢ Target modules: {len(lora_config.target_modules)} layers\")\n",
    "    print(f\"    - Attention: q_proj, v_proj, k_proj, o_proj\")\n",
    "    print(f\"    - MLP: gate_proj, up_proj, down_proj\\n\")\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable_pct = trainable / total * 100\n",
    "    \n",
    "    logger.info(f\"‚úÖ Model wrapped with LoRA\")\n",
    "    logger.info(f\"  ‚Ä¢ Trainable: {trainable / 1e6:.2f}M ({trainable_pct:.2f}%)\")\n",
    "    logger.info(f\"  ‚Ä¢ Total: {total / 1e9:.2f}B\")\n",
    "    \n",
    "    print(f\"‚úÖ LoRA adapter configured\")\n",
    "    print(f\"Trainable parameters: {trainable / 1e6:.2f}M ({trainable_pct:.2f}%)\")\n",
    "    print(f\"Total parameters: {total / 1e9:.2f}B\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b66106",
   "metadata": {},
   "source": [
    "### ‚ö° Optional: Install Unsloth for 2-3x Faster Training\n",
    "\n",
    "Run this **before cell 4** if you want to use Unsloth (2-3x speedup, 40% less VRAM):\n",
    "\n",
    "```python\n",
    "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "```\n",
    "\n",
    "Then in cell 4, change: `USE_UNSLOTH = True`\n",
    "\n",
    "**Comparison**:\n",
    "- PEFT: 1.5h training (5K samples) ‚Üí 1h saving + checkpoint\n",
    "- Unsloth: 30min training (5K samples) ‚Üí Much faster! ‚ö°\n",
    "\n",
    "Choose based on your time: if you want quick test, use Unsloth!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77730c1a",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Fine-tune Model with Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Setup output directory\n",
    "output_dir = \"/kaggle/working/qwen7b_waf_adapter\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ‚öôÔ∏è TRAINING CONFIGURATION - CUSTOMIZE HERE\n",
    "NUM_EPOCHS = 2  # ‚Üê INCREASE TO 3-4 FOR BETTER QUALITY (slower), REDUCE TO 1 FOR QUICK TEST\n",
    "TRAIN_BATCH_SIZE = AUTO_BATCH_SIZE  # Use auto-detected, or set manually (4, 8, 16, 32)\n",
    "GRADIENT_ACCUMULATION = 2\n",
    "LEARNING_RATE = 1.5e-4\n",
    "\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"üîß TRAINING CONFIGURATION\")\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(f\"  ‚Ä¢ Epochs: {NUM_EPOCHS}\")\n",
    "logger.info(f\"  ‚Ä¢ Per-device batch: {TRAIN_BATCH_SIZE}\")\n",
    "logger.info(f\"  ‚Ä¢ Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "logger.info(f\"  ‚Ä¢ Effective batch: {TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "logger.info(f\"  ‚Ä¢ Learning rate: {LEARNING_RATE}\")\n",
    "logger.info(f\"  ‚Ä¢ Output dir: {output_dir}\")\n",
    "logger.info(f\"  ‚Ä¢ Samples: {len(tokenized_dataset)}\")\n",
    "logger.info(f\"  ‚Ä¢ Model: Qwen-7B-Instruct (4-bit quantized)\")\n",
    "logger.info(f\"  ‚Ä¢ Precision: BF16 (optimal for Qwen)\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "print(\"üîß Training Configuration:\")\n",
    "print(f\"  ‚Ä¢ Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  ‚Ä¢ Batch size: {TRAIN_BATCH_SIZE}\")\n",
    "print(f\"  ‚Ä¢ Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  ‚Ä¢ Effective batch: {TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  ‚Ä¢ Learning rate: {LEARNING_RATE}\\n\")\n",
    "\n",
    "# Training arguments optimized for Kaggle GPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    report_to=[],\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "logger.info(f\"üöÄ Initializing trainer...\")\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Samples: {len(tokenized_dataset)}\")\n",
    "print(f\"Steps per epoch: {len(tokenized_dataset) // (TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION)}\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Log trainer info\n",
    "steps_per_epoch = len(tokenized_dataset) // (TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION)\n",
    "total_steps = steps_per_epoch * NUM_EPOCHS\n",
    "logger.info(f\"‚úÖ Trainer initialized\")\n",
    "logger.info(f\"  ‚Ä¢ Steps per epoch: {steps_per_epoch}\")\n",
    "logger.info(f\"  ‚Ä¢ Total training steps: {total_steps}\")\n",
    "\n",
    "# Train\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"‚è≥ TRAINING IN PROGRESS...\")\n",
    "logger.info(\"=\"*60)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚è≥ Training in progress...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"‚úÖ TRAINING COMPLETED!\")\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "logger.info(f\"Training steps: {train_result.global_step}\")\n",
    "logger.info(f\"Training time: {train_result.training_steps / 60:.1f} minutes\")\n",
    "logger.info(f\"Average loss per epoch: {train_result.training_loss / NUM_EPOCHS:.4f}\")\n",
    "logger.info(f\"Samples per second: {train_result.metrics.get('train_samples_per_second', 'N/A')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training completed!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training steps: {train_result.global_step}\")\n",
    "print(f\"Training time: {train_result.training_steps / 60:.1f} minutes\")\n",
    "print(f\"Average loss per epoch: {train_result.training_loss / NUM_EPOCHS:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c072c228",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Save and Compress LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e51cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"üíæ Saving LoRA adapter...\")\n",
    "print(\"üíæ Saving LoRA adapter...\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "logger.info(f\"‚úÖ Adapter saved to: {output_dir}\")\n",
    "print(f\"‚úÖ Adapter saved to: {output_dir}\")\n",
    "\n",
    "# List files\n",
    "logger.info(\"üìÅ Files saved:\")\n",
    "print(\"\\nüìÅ Files saved:\")\n",
    "for file in os.listdir(output_dir):\n",
    "    file_path = os.path.join(output_dir, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / 1e6\n",
    "        logger.info(f\"  ‚Ä¢ {file} ({size_mb:.2f} MB)\")\n",
    "        print(f\"  ‚Ä¢ {file} ({size_mb:.2f} MB)\")\n",
    "\n",
    "logger.info(\"\\nüì¶ Compressing adapter...\")\n",
    "print(\"\\nüì¶ Compressing adapter...\")\n",
    "\n",
    "# Compress to zip\n",
    "zip_name = f\"qwen7b_waf_adapter_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "zip_path = shutil.make_archive(\n",
    "    f\"/kaggle/working/{zip_name}\",\n",
    "    'zip',\n",
    "    output_dir\n",
    ")\n",
    "\n",
    "zip_size_mb = os.path.getsize(zip_path) / 1e6\n",
    "logger.info(f\"‚úÖ Compressed: {zip_name}.zip ({zip_size_mb:.2f} MB)\")\n",
    "print(f\"‚úÖ Compressed to: {zip_name}.zip ({zip_size_mb:.2f} MB)\")\n",
    "\n",
    "# Copy to output directory for download\n",
    "output_zip = f\"/kaggle/working/{zip_name}.zip\"\n",
    "logger.info(f\"üì• Adapter ready: {output_zip}\")\n",
    "print(f\"\\nüì• Adapter ready for download at: {output_zip}\")\n",
    "\n",
    "# Summary\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"üéâ TRAINING COMPLETE!\")\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(f\"Adapter file: {zip_name}.zip | Size: {zip_size_mb:.2f} MB\")\n",
    "logger.info(f\"Model: Qwen-7B-Instruct | Samples: {len(tokenized_dataset)} | Epochs: 2\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAdapter file: {zip_name}.zip\")\n",
    "print(f\"Size: {zip_size_mb:.2f} MB\")\n",
    "print(f\"Model: Qwen-7B-Instruct\")\n",
    "print(f\"Samples trained: {len(tokenized_dataset)}\")\n",
    "print(f\"Epochs: 2\")\n",
    "print(f\"\\nDownload the zip file and extract it with your Qwen model!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "logger.info(\"\\nüìä Logs saved to: {log_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d521b2e",
   "metadata": {},
   "source": [
    "## üß™ (Optional) Quick Test Inference\n",
    "\n",
    "Test the adapter with a sample prompt before downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25981b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"üß™ Starting inference test with trained adapter...\")\n",
    "print(\"üß™ Testing inference with trained adapter...\\n\")\n",
    "\n",
    "# Test prompt - EXACT Qwen format\n",
    "test_prompt = \"\"\"<|im_start|>system\n",
    "You are a helpful cybersecurity assistant specialized in WAF evasion techniques and SQL injection/XSS payload generation.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Generate a SQL injection payload that bypasses WAF detection by using comments and encoding.\n",
    "Target: SELECT * FROM users WHERE id=1\n",
    "WAF blocking: UNION, SELECT, --, /*\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f\"Test prompt length: {len(test_prompt)} chars\")\n",
    "print(f\"üìù Test prompt (Qwen format):\")\n",
    "print(\"=\" * 60)\n",
    "print(test_prompt)\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚è≥ Generating response...\\n\")\n",
    "\n",
    "try:\n",
    "    # Tokenize and generate using the trained model (not inference_model)\n",
    "    logger.info(\"Tokenizing test prompt...\")\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    logger.info(f\"Input tokens: {inputs['input_ids'].shape}\")\n",
    "    \n",
    "    logger.info(\"Generating response with trained model...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    logger.info(f\"Generated tokens: {outputs.shape}\")\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    logger.info(\"Response generated successfully\")\n",
    "    \n",
    "    print(f\"‚úÖ Generated response:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(response)\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    logger.info(\"‚úÖ Adapter trained and tested successfully!\")\n",
    "    print(\"üéâ Adapter trained and tested successfully!\")\n",
    "    print(f\"Format: ‚úì Qwen-7B exact format\")\n",
    "    logger.info(f\"Model dtype: {model.dtype}\")\n",
    "    logger.info(f\"Response length: {len(response)} chars\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Inference failed: {e}\", exc_info=True)\n",
    "    print(f\"‚ùå Inference failed: {e}\")\n",
    "    print(\"This is expected if model hasn't been trained yet.\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
