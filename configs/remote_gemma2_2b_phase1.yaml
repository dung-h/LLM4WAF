# Gemma 2 2B Phase 1 - RTX 4090 Optimized
# Basic SFT training without observations
# Uses balanced 10k subset with 509 diverse techniques

model_name: "google/gemma-2-2b-it"
train_path: "data/processed/phase1_balanced_10k.jsonl"
output_dir: "experiments/remote_gemma2_2b_phase1"

# Training - RTX 4090 (24GB VRAM)
num_train_epochs: 3
per_device_train_batch_size: 2 # Can handle 2 on 4090
gradient_accumulation_steps: 8 # Effective batch = 16
learning_rate: 2.0e-4
max_seq_length: 1024 # Increased from 512 to avoid truncation
weight_decay: 0.01

# 4-bit Quantization
load_in_4bit: true
bnb_4bit_quant_type: "nf4"
bnb_4bit_use_double_quant: true
bnb_4bit_compute_dtype: "float16"

# LoRA Config
lora_r: 16 # Increased capacity
lora_alpha: 32
target_modules:
  ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
lora_dropout: 0.05

# Optimization
optim: "paged_adamw_8bit"
gradient_checkpointing: true
fp16: true
bf16: false
warmup_ratio: 0.03
lr_scheduler_type: "cosine"

# Data Loading (4090 has good CPU/RAM)
dataloader_num_workers: 8
dataloader_prefetch_factor: 4
dataloader_pin_memory: true
group_by_length: true

# Logging & Checkpoints
logging_steps: 10
save_steps: 500
eval_steps: 500
save_total_limit: 3
report_to: ["tensorboard"]

use_auth_token_env: "HF_TOKEN"
