model_name: "google/gemma-2-2b-it"
train_path: "data/processed/red_phase3_lightweight.jsonl"
output_dir: "experiments/red_phase3_lightweight_gemma"
num_train_epochs: 2
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 2.0e-5
max_seq_length: 512
load_in_4bit: true
bnb_4bit_quant_type: "nf4"
bnb_4bit_use_double_quant: true
bnb_4bit_compute_dtype: "float16"
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
logging_steps: 10
save_steps: 100
save_total_limit: 2
gradient_checkpointing: true
use_auth_token_env: "HF_TOKEN"
