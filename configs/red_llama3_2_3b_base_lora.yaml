
# Configuration for Llama-3.2-3B-Instruct fine-tuning
# This file will be used by scripts/train_red.py

model_name: "meta-llama/Llama-3.2-3B"
adapter_name: "red_llama3_2_3b_lora" # Name for the LoRA adapter directory
prompt_format_type: "llama3_instruct" # Specify the prompt format for Llama 3 Instruct

# Dataset paths and fields
train_path: "data/processed/red_train_v6_multi_clean_small.jsonl"
eval_path: "data/processed/red_test_v6_multi_clean.jsonl"
text_fields: ["instruction", "context", "constraints", "payload", "reasoning"]

# Training parameters
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
per_device_train_batch_size: 1 # Keep small for 8GB VRAM
gradient_accumulation_steps: 4 # Accumulate gradients to simulate larger batch size
learning_rate: 2e-4
num_train_epochs: 1
fp16: true # Use mixed precision training
optim: "paged_adamw_8bit" # Memory efficient optimizer
logging_steps: 10
save_steps: 50
output_dir: "experiments/red_llama3_2_3b_base_lora_adapter"
max_seq_length: 512 # Max sequence length for training
