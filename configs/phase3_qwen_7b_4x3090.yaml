# Phase 3: 7-8B Model Training Configuration for 4x RTX 3090 GPUs

model_name: "Qwen/Qwen2-7B-Instruct"
model_type: "qwen2"
base_model_revision: "main"

# === 4x RTX 3090 GPU OPTIMIZATION ===
# RTX 3090: 24GB VRAM per GPU = 96GB total
# 7B model: ~14GB, leaving 10GB per GPU for batch processing

training:
  # Multi-GPU Configuration
  num_gpus: 4
  device_map: "auto"
  torch_dtype: "float16"

  # Optimized Batch Configuration for 4x RTX 3090
  per_device_train_batch_size: 10 # 10 samples per GPU (aggressive)
  gradient_accumulation_steps: 4 # Total effective batch = 10 * 4 * 4 = 160
  per_device_eval_batch_size: 12 # Higher for evaluation

  # Advanced Memory Optimization
  gradient_checkpointing: true
  dataloader_pin_memory: true
  group_by_length: true
  max_sequence_length: 2048

  # Training Schedule
  num_epochs: 2 # Increased for 7B scale
  learning_rate: 5e-6 # Lower LR for larger model
  warmup_ratio: 0.05
  weight_decay: 0.01

  # Performance Optimization
  fp16: true
  bf16: false # RTX 3090 optimized for fp16
  remove_unused_columns: false

  # Logging & Checkpointing
  logging_steps: 50
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  evaluation_strategy: "steps"

# === LORA CONFIGURATION ===
lora:
  r: 64 # Higher rank for 7B model
  lora_alpha: 128 # Scaled alpha
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "up_proj"
    - "down_proj"
    - "gate_proj"
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

# === DEEPSPEED STAGE 2 ===
deepspeed:
  stage: 2
  enable_mixed_precision: true
  reduce_scatter: true
  overlap_comm: true
  contiguous_gradients: true
  sub_group_size: 1000000000
  reduce_bucket_size: 500000000
  stage3_prefetch_bucket_size: 50000000
  stage3_param_persistence_threshold: 100000

# === DATA CONFIGURATION ===
data:
  train_file: "data/processed/train_full_qwen_cleaned.jsonl"
  eval_file: "data/splits/sft_experiment/test_200_qwen.jsonl"
  max_samples: 11589 # Full clean dataset

# === OUTPUT CONFIGURATION ===
output:
  output_dir: "experiments/qwen_7b_4gpu_phase3"
  run_name: "qwen_7b_4x3090_phase3"
  project_name: "llm4waf_phase3"

# === QUALITY TARGETS ===
targets:
  quality_score: 0.55 # Target improvement from 0.463
  empty_outputs: 0.0 # Maintain zero empty
  conversational: 0.0 # Maintain zero conversational
  training_time_hours: 8 # Expected on 4x RTX 3090
