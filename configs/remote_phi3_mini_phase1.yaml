# Phi-3 Mini Phase 1 - 16GB VRAM with DynamicCache Fix
# Basic SFT training without observations
# Uses balanced 10k subset with 509 diverse techniques

model_name: "microsoft/Phi-3-mini-4k-instruct"
train_path: "data/processed/phase1_balanced_10k.jsonl"
output_dir: "experiments/remote_phi3_mini_phase1"

# Training - 16GB VRAM
num_train_epochs: 3
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 2.0e-4
max_seq_length: 1024
weight_decay: 0.01

# 4-bit Quantization
load_in_4bit: true
bnb_4bit_quant_type: "nf4"
bnb_4bit_use_double_quant: true
bnb_4bit_compute_dtype: "float16"

# LoRA Config
lora_r: 16
lora_alpha: 32
target_modules:
  ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
lora_dropout: 0.05

# Optimization - CRITICAL for Phi-3
optim: "paged_adamw_8bit"
gradient_checkpointing: true # Must enable
fp16: true
bf16: false
warmup_ratio: 0.03
lr_scheduler_type: "cosine"

# Data Loading
dataloader_num_workers: 8
dataloader_prefetch_factor: 4
dataloader_pin_memory: true
group_by_length: true

# Logging & Checkpoints
logging_steps: 10
save_steps: 500
eval_steps: 500
save_total_limit: 3
report_to: ["tensorboard"]

use_auth_token_env: "HF_TOKEN"
# NOTE: DynamicCache fix is in train_red.py
# Model will automatically disable cache when gradient_checkpointing=true
