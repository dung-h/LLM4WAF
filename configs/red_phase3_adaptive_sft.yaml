# Model Configuration
# Start from Phase 2 Adapter to retain reasoning capabilities
# For Phi-3: experiments/remote_adapters/experiments_remote_optimized/phase2_reasoning_phi3
# For Qwen: experiments/remote_adapters/experiments_remote_optimized/phase2_reasoning_qwen
model_name: "experiments/remote_adapters/experiments_remote_optimized/phase2_reasoning_phi3" 

# Dataset
train_path: "data/processed/red_phase3_adaptive_sft.jsonl"
output_dir: "experiments/red_phase3_adaptive_phi3"

# Training Hyperparameters
num_train_epochs: 2
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 16 # Effective batch size = 16
learning_rate: 1.0e-5 # Low LR for fine-tuning
max_seq_length: 1024 # Sufficient for prompt + history + payload
lr_scheduler_type: "cosine"
warmup_ratio: 0.03

# QLoRA Configuration
load_in_4bit: true
bnb_4bit_quant_type: "nf4"
bnb_4bit_use_double_quant: true
bnb_4bit_compute_dtype: "float16"

# LoRA Configuration
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules: 
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

# Logging & Saving
logging_steps: 10
save_steps: 100
save_total_limit: 2
use_auth_token_env: "HF_TOKEN"
