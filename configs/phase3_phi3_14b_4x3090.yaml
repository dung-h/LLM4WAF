# Phase 3: Phi-3-Medium-14B Configuration for 4x RTX 3090 GPUs

model_name: "microsoft/Phi-3-medium-14b-instruct"
model_type: "phi3"
base_model_revision: "main"

# === 4x RTX 3090 GPU OPTIMIZATION ===
training:
  # Multi-GPU Configuration
  num_gpus: 4
  device_map: "auto"
  torch_dtype: "float16"

  # Conservative Batch for 14B Model
  per_device_train_batch_size: 3 # 3 samples per GPU (max safe for 14B)
  gradient_accumulation_steps: 11 # Total effective batch = 3 * 4 * 11 = 132
  per_device_eval_batch_size: 4

  # Aggressive Memory Optimization
  gradient_checkpointing: true
  dataloader_pin_memory: true
  group_by_length: true
  max_sequence_length: 1536 # Reduced for 14B model

  # Training Schedule
  num_epochs: 1 # Single epoch for 14B
  learning_rate: 2e-6 # Very conservative LR
  warmup_ratio: 0.03
  weight_decay: 0.01

  # Performance
  fp16: true
  remove_unused_columns: false

  # Logging
  logging_steps: 50
  save_steps: 1000 # Less frequent saves
  eval_steps: 1000
  save_total_limit: 2
  evaluation_strategy: "steps"

# === LORA CONFIGURATION ===
lora:
  r: 32 # Lower rank for 14B model memory
  lora_alpha: 64
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_up_proj"
    - "down_proj"
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

# === DATA ===
data:
  train_file: "data/processed/train_full_phi3_cleaned.jsonl"
  eval_file: "data/splits/sft_experiment/test_200_phi3.jsonl"
  max_samples: 11589

# === OUTPUT ===
output:
  output_dir: "experiments/phi3_14b_4gpu_phase3"
  run_name: "phi3_14b_4x3090_phase3"
  project_name: "llm4waf_phase3"
