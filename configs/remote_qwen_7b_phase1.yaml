# Qwen 2.5 7B Phase 1 - 16GB VRAM
# Larger model, requires more careful memory management
# Uses balanced 10k subset with 509 diverse techniques

model_name: "Qwen/Qwen2.5-7B-Instruct"
train_path: "data/processed/phase1_balanced_10k.jsonl"
output_dir: "experiments/remote_qwen_7b_phase1"

# Training - 16GB VRAM (7B needs batch_size=1)
num_train_epochs: 3
per_device_train_batch_size: 1 # Required for 7B on 16GB
gradient_accumulation_steps: 16 # Effective batch = 16
learning_rate: 2.0e-4
max_seq_length: 1024
weight_decay: 0.01

# 4-bit Quantization (ESSENTIAL for 7B)
load_in_4bit: true
bnb_4bit_quant_type: "nf4"
bnb_4bit_use_double_quant: true
bnb_4bit_compute_dtype: "float16"

# LoRA Config
lora_r: 16
lora_alpha: 32
target_modules:
  ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
lora_dropout: 0.05

# Optimization
optim: "paged_adamw_8bit"
gradient_checkpointing: true
fp16: true
bf16: false
warmup_ratio: 0.03
lr_scheduler_type: "cosine"

# Data Loading
dataloader_num_workers: 8
dataloader_prefetch_factor: 4
dataloader_pin_memory: true
group_by_length: true

# Logging & Checkpoints
logging_steps: 10
save_steps: 500
eval_steps: 500
save_total_limit: 2 # Save space for 7B
report_to: ["tensorboard"]

use_auth_token_env: "HF_TOKEN"
