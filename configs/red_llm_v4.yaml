# v4 Training Configuration (NO CONSTRAINTS + SIMPLIFIED)
# Based on successful v3 config, but with v4 dataset

model_name: google/gemma-2-2b-it
output_dir: experiments/red_v4

# v4 Dataset (simplified, no constraints)
train_path: data/processed/red_train_v4.jsonl
eval_path: data/processed/red_val_v4.jsonl
test_path: data/processed/red_test_v4.jsonl

# Training hyperparameters (PROVEN from v3)
num_train_epochs: 3
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 2.0e-4
lr_scheduler_type: cosine
warmup_ratio: 0.1
weight_decay: 0.01

# Memory optimization (8GB GPU)
gradient_checkpointing: true
optim: paged_adamw_8bit

# LoRA config
use_lora: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

# Quantization (4-bit for memory efficiency)
load_in_4bit: true
bnb_4bit_compute_dtype: float16
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true

# Data processing
max_seq_length: 512
packing: false

# Evaluation
eval_strategy: steps
eval_steps: 50
save_strategy: steps
save_steps: 50
save_total_limit: 3
logging_steps: 10

# Performance
fp16: true
tf32: true
dataloader_num_workers: 4
remove_unused_columns: false

# Early stopping
metric_for_best_model: eval_loss
greater_is_better: false
load_best_model_at_end: true

# Reproducibility
seed: 42
# v4 SPECIFIC NOTES:
# - Dataset has NO "constraints" field
# - Focus on "Bypass WAF" aggressive mindset
# - Simpler payloads (avg ~40 chars vs v3's 58)
# - Filtered out complex placeholders
# - Expected: Better bypass rate than v3
