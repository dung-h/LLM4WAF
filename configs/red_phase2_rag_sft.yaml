# Model
model_name: "google/gemma-2-2b-it"
model_type: "gemma"

# Data
train_path: "data/processed/red_phase2_rag_sft_train.jsonl"
dataset_format: "gemma" 

# Training
output_dir: "experiments/red_gemma2_2b_rag_sft_v1"
num_train_epochs: 3
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 2.0e-4
logging_steps: 50
save_steps: 500
max_seq_length: 512

# LoRA
use_lora: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
