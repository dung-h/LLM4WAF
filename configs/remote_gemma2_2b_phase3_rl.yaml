# Gemma 2 2B Phase 3 RL Training (REINFORCE)
# Continue from Phase 2 remote adapter with live WAF interaction
# For 16GB VRAM remote training

# Model
base_model: "google/gemma-2-2b-it"
adapter_path: "experiments/remote_gemma2_2b_phase2" # Load Phase 2 adapter
model_type: "gemma"

# Training
output_dir: "experiments/remote_gemma2_2b_phase3_rl"
epochs: 150
batch_size: 1
lr: 1e-6
max_new_tokens: 256 # Cover 95% of payloads (95th percentile = 240 tokens)
max_context_length: 1024 # Keep same as Phase 2 for consistency
gradient_checkpointing: true # Enable to save VRAM

# WAF Configuration
waf_url: "http://modsec.llmshield.click" # Remote WAF with authentication

use_auth_token_env: "HF_TOKEN"
