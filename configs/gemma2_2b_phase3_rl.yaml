# Gemma 2 2B RL Training from Phase 2 Remote Adapter
# Continue training on Phase 2 adapter from remote training
# Uses train_rl_reinforce.py script with remote DVWA

# Model
base_model: "google/gemma-2-2b-it"
adapter_path: "experiments/remote_gemma2_2b_phase2" # Phase 2 remote adapter
model_type: "gemma"

# Training
output_dir: "experiments/gemma2_2b_phase3_rl"
epochs: 150 # Increased for better convergence
batch_size: 1
lr: 1e-6
max_new_tokens: 64
max_context_length: 384 # Reduced from 512 to prevent OOM during RL training

# WAF Configuration
waf_url: "http://modsec.llmshield.click" # Remote WAF with authentication

use_auth_token_env: "HF_TOKEN"
