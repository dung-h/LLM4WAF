model_name: google/gemma-2-2b-it
use_auth_token_env: HF_TOKEN
load_in_4bit: true
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true
bnb_4bit_compute_dtype: float16
use_dora: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules:
- q_proj
- k_proj
- v_proj
- o_proj
- gate_proj
- up_proj
- down_proj
seq_length: 1024
padding_side: right
truncation: true
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
num_train_epochs: 1
max_steps: 60
learning_rate: 0.0002
warmup_ratio: 0.03
fp16: true
logging_steps: 10
save_steps: 60
eval_steps: 0
save_total_limit: 1
train_path: /mnt/c/Users/HAD/Desktop/AI_in_cyber/LLM_in_Cyber/data/processed/red_train_v6_small_loop_c1.jsonl
text_fields:
  instruction: instruction
  context: context
  constraints: constraints
  payload: payload
  reasoning: reasoning
output_dir: /mnt/c/Users/HAD/Desktop/AI_in_cyber/LLM_in_Cyber/experiments/red_gemma2_v6_loop_c1
seed: 42
