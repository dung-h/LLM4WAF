model_name: "togethercomputer/RedPajama-INCITE-Base-3B-v1"
train_path: "processed/red_train_v6_multi_clean.jsonl"
num_train_epochs: 1
output_dir: "experiments/red_pajama_3b_sqli_finetuned_observable"

# LoRA configuration
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
target_modules: ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
use_dora: True

# BitsAndBytes configuration
load_in_4bit: True
bnb_4bit_quant_type: "nf4"
bnb_4bit_use_double_quant: True
bnb_4bit_compute_dtype: "float16"

# Training arguments
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
num_train_epochs: 3
learning_rate: 2e-4
weight_decay: 0.0
warmup_ratio: 0.03
lr_scheduler_type: "cosine"
fp16: True
bf16: False # Set to True if your GPU supports bfloat16
logging_steps: 10
save_steps: 200
eval_steps: 200
save_total_limit: 2
optim: "paged_adamw_8bit"
gradient_checkpointing: True
seq_length: 2048
padding_side: "left"

# Text fields for dataset formatting
text_fields:
  instruction: "instruction"
  context: "context"
  payload: "payload"
  reasoning: "reasoning"
prompt_format_type: "default" # Changed to default