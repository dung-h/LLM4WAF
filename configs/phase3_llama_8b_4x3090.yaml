# Phase 3: LLaMA-3-8B Configuration for 4x RTX 3090 GPUs

model_name: "meta-llama/Meta-Llama-3-8B-Instruct"
model_type: "llama"
base_model_revision: "main"

# === 4x RTX 3090 GPU OPTIMIZATION ===
training:
  # Multi-GPU Configuration
  num_gpus: 4
  device_map: "auto"
  torch_dtype: "float16"

  # Batch Configuration (8B model requires more memory)
  per_device_train_batch_size: 6 # 6 samples per GPU (pushed to limit)
  gradient_accumulation_steps: 6 # Total effective batch = 6 * 4 * 6 = 144
  per_device_eval_batch_size: 8

  # Memory Optimization
  gradient_checkpointing: true
  dataloader_pin_memory: true
  group_by_length: true
  max_sequence_length: 2048

  # Training Schedule
  num_epochs: 2
  learning_rate: 3e-6 # Conservative for 8B model
  warmup_ratio: 0.05
  weight_decay: 0.01

  # Performance
  fp16: true
  remove_unused_columns: false

  # Logging
  logging_steps: 50
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  evaluation_strategy: "steps"

# === LORA CONFIGURATION ===
lora:
  r: 64
  lora_alpha: 128
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

# === DATA ===
data:
  train_file: "data/processed/train_full_llama_cleaned.jsonl"
  eval_file: "data/splits/sft_experiment/test_200_llama.jsonl"
  max_samples: 11589

# === OUTPUT ===
output:
  output_dir: "experiments/llama_8b_4gpu_phase3"
  run_name: "llama_8b_4x3090_phase3"
  project_name: "llm4waf_phase3"
