model_name: google/gemma-2-2b-it
use_auth_token_env: HF_TOKEN

# Quantization (QLoRA 4-bit)
load_in_4bit: true
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true
bnb_4bit_compute_dtype: float16

# PEFT (LoRA)
use_dora: false
lora_r: 8
lora_alpha: 32
lora_dropout: 0.05
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

# Tokenization / sequence
seq_length: 512
padding_side: left
truncation: true

# Training params
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 1
num_train_epochs: 1
learning_rate: 0.0002
weight_decay: 0.0
warmup_ratio: 0.03
lr_scheduler_type: cosine
fp16: true
bf16: false
gradient_checkpointing: true
optim: paged_adamw_8bit
logging_steps: 1
save_steps: 10
save_total_limit: 2

# Data
train_path: data/processed/dummy_train.jsonl

# Text formatting
dataset_text_field: "text"

# Output
output_dir: experiments/red_gemma2_2b_lora_v2_debug
logging_dir: experiments/red_gemma2_2b_lora_v2_debug/logs

# Reproducibility
seed: 42
