model_name: meta-llama/Meta-Llama-3.1-8B-Instruct
use_auth_token_env: HF_TOKEN

# Quantization (QLoRA 4-bit)
load_in_4bit: true
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true
bnb_4bit_compute_dtype: float16

# PEFT (LoRA)
use_dora: false
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

# Tokenization
seq_length: 2048
padding_side: left
truncation: true

# Training params (RTX 4080 16GB)
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
learning_rate: 2e-4
num_train_epochs: 3
max_steps: -1
warmup_steps: 100
logging_steps: 10
save_steps: 500
evaluation_strategy: no
save_strategy: steps
optim: paged_adamw_8bit
max_grad_norm: 1.0
weight_decay: 0.01
lr_scheduler_type: cosine
remove_unused_columns: false
dataloader_num_workers: 4
fp16: true
report_to: []
seed: 42
dataloader_pin_memory: false
group_by_length: false
length_column_name: length