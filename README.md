# LLM4WAF: Há»‡ Thá»‘ng Red & Blue Teaming Tá»± Äá»™ng Cho Web Application Firewalls

Dá»± Ã¡n nÃ y triá»ƒn khai má»™t framework khÃ©p kÃ­n (end-to-end) cho **Táº¥n cÃ´ng Äá»‘i khÃ¡ng (Red Team)** vÃ  **Tinh chá»‰nh PhÃ²ng thá»§ ThÃ´ng minh (Blue Team)** sá»­ dá»¥ng cÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLMs).

## ğŸš€ Tá»•ng Quan Dá»± Ãn

Má»¥c tiÃªu lÃ  tá»± Ä‘á»™ng hÃ³a quy trÃ¬nh tÃ¬m kiáº¿m lá»— há»•ng (bypass WAF) vÃ  vÃ¡ chÃºng:
1.  **Red Agent (Táº¥n cÃ´ng):** Sá»­ dá»¥ng Há»c tÄƒng cÆ°á»ng (Reinforcement Learning - RL) Ä‘á»ƒ sinh ra cÃ¡c payload SQL Injection (SQLi) vÃ  XSS tinh vi nháº±m vÆ°á»£t qua WAF.
2.  **Blue Agent (PhÃ²ng thá»§):** PhÃ¢n tÃ­ch cÃ¡c cuá»™c táº¥n cÃ´ng thÃ nh cÃ´ng báº±ng RAG (Retrieval-Augmented Generation) vÃ  cÆ¡ sá»Ÿ tri thá»©c OWASP Core Rule Set (CRS) Ä‘á»ƒ Ä‘á» xuáº¥t cáº¥u hÃ¬nh WAF chÃ­nh xÃ¡c.

---

## ğŸ› ï¸ CÃ i Äáº·t MÃ´i TrÆ°á»ng

### 1. YÃªu cáº§u tiÃªn quyáº¿t
*   **OS:** Linux (KhuyÃªn dÃ¹ng WSL2 trÃªn Windows).
*   **Python:** 3.10+.
*   **Docker & Docker Compose:** Báº¯t buá»™c Ä‘á»ƒ cháº¡y cÃ¡c WAF vÃ  á»©ng dá»¥ng má»¥c tiÃªu (DVWA, Juice Shop).
*   **GPU:** NVIDIA GPU (KhuyÃªn dÃ¹ng 16GB+ VRAM) Ä‘á»ƒ train vÃ  inference LLM cá»¥c bá»™.

### 2. CÃ i Ä‘áº·t
```bash
# 1. Clone repository
git clone <repo_url>
cd LLM_in_Cyber

# 2. Táº¡o mÃ´i trÆ°á»ng áº£o
python -m venv .venv
source .venv/bin/activate

# 3. CÃ i Ä‘áº·t thÆ° viá»‡n
pip install -r requirements.txt
```

---

## ğŸ”´ RED Agent Pipeline (Äá»™i Táº¥n CÃ´ng)

Red Agent tiáº¿n hÃ³a tá»« má»™t bá»™ sinh payload cÆ¡ báº£n thÃ nh má»™t cÃ´ng cá»¥ nÃ© trÃ¡nh thÃ´ng minh qua 3 giai Ä‘oáº¡n.

### Phase 1: Supervised Fine-Tuning (SFT) - Há»c CÃº PhÃ¡p
**Má»¥c tiÃªu:** Dáº¡y model náº¯m vá»¯ng cÃº phÃ¡p cá»§a cÃ¡c payload SQLi vÃ  XSS há»£p lá»‡ Ä‘Ã£ tá»«ng bypass thÃ nh cÃ´ng.
*   **Dá»¯ liá»‡u Ä‘áº§u vÃ o:** `data/processed/red_v40_passed_waf_only.jsonl` (Táº­p há»£p cÃ¡c payload bypass thÃ nh cÃ´ng).
*   **1. Huáº¥n luyá»‡n (Training):**
    ```bash
    python scripts/train_red.py --config configs/red_gemma2_2b_lora_v2.yaml
    ```
    *   *Output:* Checkpoint model táº¡i `experiments/red_gemma2_2b_lora_v2`.
*   **2. Kiá»ƒm thá»­ (Testing):**
    ÄÃ¡nh giÃ¡ kháº£ nÄƒng sinh payload há»£p lá»‡ trÃªn táº­p test tÄ©nh.
    ```bash
    python scripts/evaluate_model.py \
      --base_model google/gemma-2-2b-it \
      --adapter experiments/red_gemma2_2b_lora_v2 \
      --dataset data/processed/red_v40_test_200.jsonl \
      --format gemma
    ```

### Phase 2: Reasoning SFT (Chain-of-Thought) - Há»c TÆ° Duy
**Má»¥c tiÃªu:** Cáº£i thiá»‡n kháº£ nÄƒng thÃ­ch á»©ng báº±ng cÃ¡ch dáº¡y model *cÃ¡ch suy nghÄ©* vá» viá»‡c nÃ© trÃ¡nh (Reasoning Traces).
*   **Dá»¯ liá»‡u Ä‘áº§u vÃ o:** `data/processed/red_v40_phase2_reasoning.jsonl` (Bá»™ ba: Lá»‹ch sá»­ -> Suy luáº­n -> Payload má»›i).
*   **1. Huáº¥n luyá»‡n (Training):**
    ```bash
    # (Optional) Format dataset
    # python scripts/build_phase2_dataset.py 
    
    python scripts/train_red.py --config configs/phase2_gemma2_2b_reasoning.yaml
    ```
    *   *Output:* Checkpoint model táº¡i `experiments/phase2_gemma2_2b_reasoning`.
*   **2. Kiá»ƒm thá»­ (Testing):**
    Kiá»ƒm tra model cÃ³ sinh ra chuá»—i suy luáº­n (reasoning) há»£p lÃ½ trÆ°á»›c khi táº¡o payload khÃ´ng.
    ```bash
    python scripts/evaluate_model.py \
      --base_model google/gemma-2-2b-it \
      --adapter experiments/phase2_gemma2_2b_reasoning \
      --dataset data/processed/red_v40_phase2_eval_test.jsonl \
      --format gemma
    ```

### Phase 3: Reinforcement Learning (RL) - Tá»‘i Æ¯u HÃ³a
**Má»¥c tiÃªu:** Tá»‘i Ä‘a hÃ³a tá»· lá»‡ Bypass WAF vÃ  Thá»±c thi thÃ nh cÃ´ng thÃ´ng qua tÆ°Æ¡ng tÃ¡c Thá»­ & Sai.
*   **Äáº§u vÃ o:** 
    *   **Model khá»Ÿi táº¡o:** Load tá»« Phase 2 (`experiments/phase2_gemma2_2b_reasoning`).
    *   **MÃ´i trÆ°á»ng:** Docker container cá»¥c bá»™ (`WafEnv`) cháº¡y ModSecurity/Coraza.
*   **1. Huáº¥n luyá»‡n (Training):**
    ```bash
    # Khá»Ÿi Ä‘á»™ng mÃ´i trÆ°á»ng WAF trÆ°á»›c
    docker-compose -f docker-compose.multiwaf.yml up -d
    
    # Cháº¡y RL Training loop
    python scripts/train_rl_reinforce.py --epochs 25 --batch_size 2
    ```
    *   *Output:* Model hoÃ n thiá»‡n táº¡i `experiments/phase3_gemma2_2b_rl`.
*   **2. Kiá»ƒm thá»­ (Testing - Táº¥n cÃ´ng thá»±c táº¿):**
    Sá»­ dá»¥ng model RL Ä‘á»ƒ táº¥n cÃ´ng vÃ o cÃ¡c WAF má»¥c tiÃªu vÃ  Ä‘o tá»· lá»‡ bypass.
    ```bash
    python scripts/run_red_eval_profile.py \
      --config configs/eval_phase3_multiwaf_gemma2.yaml \
      --num_samples 50
    ```

---

## ğŸ”µ BLUE Agent Pipeline (Äá»™i PhÃ²ng Thá»§)

Blue Agent Ä‘Ã³ng vai trÃ² lÃ  má»™t ChuyÃªn gia An ninh AI Ä‘á»ƒ tinh chá»‰nh WAF dá»±a trÃªn dá»¯ liá»‡u tá»« Red Team.

### Phase 1: Chuáº©n Bá»‹ Dá»¯ Liá»‡u & Knowledge Base
**Má»¥c tiÃªu:** Chuáº©n bá»‹ dá»¯ liá»‡u cho AI Analyst.
*   **Äáº§u vÃ o:** Log táº¥n cÃ´ng tá»« Red Team (format JSONL).
*   **Quy trÃ¬nh:**
    1.  **Episodes:** Chuyá»ƒn Ä‘á»•i log thÃ´ thÃ nh "Episodes" cÃ³ cáº¥u trÃºc (Attack + WAF Response + App Response).
    2.  **Knowledge Base:** Index tÃ i liá»‡u OWASP CRS (regex rules, tags) vÃ o vector store.
*   **Lá»‡nh cháº¡y:**
    ```bash
    # Build Episodes
    python scripts/blue_build_phase1_episodes.py
    
    # Build Knowledge Base
    python scripts/blue_build_crs_kb.py
    ```
*   **Dá»¯ liá»‡u Ä‘áº§u ra:** 
    *   `data/blue/blue_phase1_episodes.jsonl`
    *   `data/blue/blue_phase1_crs_kb.jsonl`

### Phase 2: RAG Analysis & Evaluation
**Má»¥c tiÃªu:** Truy xuáº¥t cÃ¡c rule liÃªn quan vÃ  kiá»ƒm chá»©ng kháº£ nÄƒng phÃ¢n tÃ­ch cá»§a AI trÃªn táº­p Golden Set.
*   **Äáº§u vÃ o:** `data/blue/blue_phase1_golden.jsonl` (CÃ¡c case Ä‘Ã£ Ä‘Æ°á»£c xÃ¡c minh).
*   **Lá»‡nh cháº¡y:**
    ```bash
    python blue/runner_phase2_eval.py
    ```
*   **Äáº§u ra:** `data/blue/blue_phase2_eval_summary.txt` (BÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c phÃ¢n tÃ­ch).

### Phase 3: Recommendation Generation (Táº¡o Äá» Xuáº¥t)
**Má»¥c tiÃªu:** Sinh ra cÃ¡c thay Ä‘á»•i cáº¥u hÃ¬nh cá»¥ thá»ƒ (Báº£n vÃ¡).
*   **Äáº§u vÃ o:** `data/blue/blue_phase1_episodes.jsonl` + RAG Knowledge Base.
*   **Quy trÃ¬nh:** Blue LLM (Sá»­ dá»¥ng Gemma 2 Base Model Ä‘á»ƒ Ä‘áº£m báº£o format JSON chuáº©n) phÃ¢n tÃ­ch tá»«ng cuá»™c táº¥n cÃ´ng thÃ nh cÃ´ng vÃ  Ä‘á» xuáº¥t rule WAF cá»¥ thá»ƒ.
*   **Lá»‡nh cháº¡y:**
    ```bash
    python blue/runner_phase3_suggest.py
    ```
*   **Äáº§u ra:** `data/blue/blue_phase3_suggestions.jsonl` (Danh sÃ¡ch JSON cÃ¡c rule Ä‘Æ°á»£c Ä‘á» xuáº¥t).

### Phase 4: WAF Overlay & Evaluation (Ãp Dá»¥ng & ÄÃ¡nh GiÃ¡)
**Má»¥c tiÃªu:** Ãp dá»¥ng báº£n vÃ¡ vÃ  kiá»ƒm tra hiá»‡u quáº£.
*   **Quy trÃ¬nh:** 
    1.  **Generate Config:** Chuyá»ƒn Ä‘á»•i JSON suggestions thÃ nh file config WAF thá»±c táº¿ (`.conf`, `.yaml`).
    2.  **Re-Eval:** Khá»Ÿi Ä‘á»™ng láº¡i WAF vá»›i config má»›i vÃ  cho Red Team táº¥n cÃ´ng láº¡i.
*   **Lá»‡nh cháº¡y:**
    ```bash
    # 1. Táº¡o file cáº¥u hÃ¬nh WAF
    python blue/phase3_generate_waf_overlays.py
    
    # 2. Khá»Ÿi Ä‘á»™ng mÃ´i trÆ°á»ng Multi-WAF
    docker-compose -f docker-compose.multiwaf.yml up -d --build
    
    # 3. Cháº¡y Ä‘Ã¡nh giÃ¡ Red Team (Kiá»ƒm tra láº¡i kháº£ nÄƒng bypass)
    python scripts/run_red_eval_profile.py --config configs/eval_phase3_multiwaf_gemma2.yaml
    ```
*   **Äáº§u ra:** 
    *   `waf/blue_modsecurity_suggestions.conf`: File chá»©a rule WAF má»›i sinh ra.
    *   `eval/red_phase4_overall_summary.json`: BÃ¡o cÃ¡o so sÃ¡nh hiá»‡u quáº£ (Base WAF vs. Blue Tuned WAF).

---

## ğŸ“Š Káº¿t Quáº£ ChÃ­nh (VÃ­ dá»¥)

ÄÃ¡nh giÃ¡ gáº§n nháº¥t trÃªn DVWA (SQL Injection):

| Profile | WAF Engine | Ruleset | Blocked % | WAF Bypass % |
| :--- | :--- | :--- | :--- | :--- |
| **Baseline** | ModSecurity | OWASP CRS PL1 | ~5.7% | 94.3% |
| **Blue Tuned** | ModSecurity | PL1 + Blue Overlay | **ÄÃ£ cáº£i thiá»‡n** | **(Má»¥c tiÃªu: < 90%)** |
| **Strict** | ModSecurity | OWASP CRS PL4 | 0% (Cáº§n check config) | 100% |

*LÆ°u Ã½: Hiá»‡u suáº¥t cá»§a "Blue Tuned" phá»¥ thuá»™c vÃ o cháº¥t lÆ°á»£ng suy luáº­n cá»§a LLM trong Phase 3.*

---

## ğŸ“‚ Cáº¥u TrÃºc Dá»± Ãn

*   `blue/`: Source code cho Blue Agent (RAG, LLM client, Prompts).
*   `configs/`: CÃ¡c file cáº¥u hÃ¬nh YAML cho training vÃ  evaluation.
*   `data/`: Dá»¯ liá»‡u (Logs Ä‘Ã£ xá»­ lÃ½, Episodes, Knowledge Base).
*   `docker-compose.multiwaf.yml`: Äá»‹nh nghÄ©a mÃ´i trÆ°á»ng Ä‘Ã¡nh giÃ¡ nhiá»u WAF.
*   `rl/`: MÃ´i trÆ°á»ng vÃ  logic cho Reinforcement Learning.
*   `scripts/`: CÃ¡c script tiá»‡n Ã­ch cho training, xá»­ lÃ½ dá»¯ liá»‡u vÃ  Ä‘Ã¡nh giÃ¡.
*   `waf/`: CÃ¡c file cáº¥u hÃ¬nh WAF overlay Ä‘Æ°á»£c sinh ra tá»± Ä‘á»™ng.

---

## ğŸ¤ Thá»±c hiá»‡n
*   **HAD** - Lead Developer / AI Security Researcher

---

## ğŸ› Known Issues / Troubleshooting

### 1. CUDA Out of Memory (OOM) on 8GB GPUs for Gemma 2B Training

*   **Váº¥n Ä‘á»:** Khi fine-tune Gemma 2 2B (ká»ƒ cáº£ vá»›i QLoRA 4-bit), GPU 8GB (vÃ­ dá»¥ RTX 3050, 3060, 4060) thÆ°á»ng gáº·p lá»—i `CUDA Out of Memory` (`torch.OutOfMemoryError`). Äiá»u nÃ y xáº£y ra ngay cáº£ khi `per_device_train_batch_size` Ä‘Ã£ giáº£m xuá»‘ng 1 vÃ  `gradient_accumulation_steps` Ä‘Ã£ tÄƒng.
*   **NguyÃªn nhÃ¢n:** Model Gemma 2 2B, dÃ¹ lÃ  2 tá»· tham sá»‘, nhÆ°ng cÃ³ kiáº¿n trÃºc phá»©c táº¡p vÃ  `max_seq_length` lá»›n (Ä‘áº·c biá»‡t cáº§n cho RAG context) Ä‘Ã²i há»i lÆ°á»£ng VRAM Ä‘Ã¡ng ká»ƒ. Cáº¥u hÃ¬nh máº·c Ä‘á»‹nh (vÃ­ dá»¥ `max_seq_length=1024`) quÃ¡ lá»›n Ä‘á»‘i vá»›i 8GB VRAM.
*   **Giáº£i phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t:**
    *   **Tá»‘t nháº¥t:** Sá»­ dá»¥ng GPU cÃ³ VRAM tá»« **16GB trá»Ÿ lÃªn** (vÃ­ dá»¥: RTX 3090/4090, A10G, A5000/6000).
    *   **Táº¡m thá»i (náº¿u chá»‰ cÃ³ 8GB VRAM):**
        *   Giáº£m `max_seq_length` trong file config (`configs/red_phase2_rag_sft.yaml`) xuá»‘ng **512 hoáº·c tháº­m chÃ­ 256**. Tuy nhiÃªn, Ä‘iá»u nÃ y sáº½ lÃ m giáº£m Ä‘Ã¡ng ká»ƒ lÆ°á»£ng RAG context mÃ  model cÃ³ thá»ƒ xá»­ lÃ½, áº£nh hÆ°á»Ÿng Ä‘áº¿n hiá»‡u quáº£ cá»§a RAG.
        *   Äáº£m báº£o `per_device_train_batch_size` lÃ  `1` vÃ  `gradient_accumulation_steps` Ä‘Æ°á»£c tÄƒng lÃªn Ä‘á»ƒ giá»¯ `effective_batch_size` há»£p lÃ½.
        *   Thá»­ táº¯t `bnb_4bit_use_double_quant` trong `BitsAndBytesConfig` (máº·c dÃ¹ script `train_red.py` Ä‘Ã£ Ä‘á»c tá»« config file, cáº§n thÃªm tÃ¹y chá»n nÃ y vÃ o config file náº¿u muá»‘n Ä‘iá»u chá»‰nh).
*   **LiÃªn quan Ä‘áº¿n RAG:** RAG-SFT ráº¥t cáº§n `max_seq_length` Ä‘á»§ lá»›n Ä‘á»ƒ chá»©a RAG context. Viá»‡c giáº£m `max_seq_length` xuá»‘ng quÃ¡ tháº¥p sáº½ lÃ m giáº£m hiá»‡u quáº£ cá»§a viá»‡c fine-tune RAG-SFT.

---

## âš ï¸ Critical Findings Regarding RED Agent Performance

Trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡ (Evaluation) cÃ¡c model RED Agent (Phase 1, 2, 3), Ä‘Ã£ phÃ¡t hiá»‡n ra má»™t yáº¿u tá»‘ cá»±c ká»³ quan trá»ng áº£nh hÆ°á»Ÿng Ä‘áº¿n hiá»‡u nÄƒng:

1.  **Prompt Sensitivity (Äá»™ nháº¡y vá»›i Prompt):**
    *   **Phase 1 (SFT):** Model nÃ y Ã­t nháº¡y cáº£m vá»›i format prompt. NÃ³ cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng tá»‘t (~55% bypass rate) vá»›i cÃ¡c prompt Ä‘Æ¡n giáº£n (e.g., "Generate payload for...").
    *   **Phase 2 (Reasoning) & Phase 3 (RL):** Hai model nÃ y **YÃŠU Cáº¦U Báº®T BUá»˜C** pháº£i sá»­ dá»¥ng Ä‘Ãºng format prompt mÃ  chÃºng Ä‘Æ°á»£c huáº¥n luyá»‡n (bao gá»“m cÃ¡c trÆ°á»ng `Context`, `Payload History`, `Target Technique`).
    *   **Thá»±c nghiá»‡m:**
        *   Sá»­ dá»¥ng prompt Ä‘Æ¡n giáº£n: Phase 2 Ä‘áº¡t ~20%, Phase 3 Ä‘áº¡t ~10%.
        *   Sá»­ dá»¥ng prompt chuáº©n (structured): Phase 2 Ä‘áº¡t **~85%**, Phase 3 Ä‘áº¡t **~90%**.

2.  **Model Size & RAG Compliance (PhÃ¢n tÃ­ch chuyÃªn sÃ¢u):**
    *   Ban Ä‘áº§u cÃ³ thá»ƒ láº§m tÆ°á»Ÿng cÃ¡c model nhá» nhÆ° Gemma 2B Ã­t tuÃ¢n thá»§ context RAG. Tuy nhiÃªn, cÃ¡c nghiÃªn cá»©u gáº§n Ä‘Ã¢y (vÃ­ dá»¥: Ghosh et al., EMNLP 2024, Farahani & Johansson, EMNLP 2024 - tham kháº£o `evidence.txt`) cho tháº¥y **cáº£ Small (nhÆ° Phi) láº«n Large LLM Ä‘á»u cÃ³ xu hÆ°á»›ng "dá»±a vÃ o context nhiá»u hÆ¡n parametric" khi context cÃ³ liÃªn quan.**
    *   Váº¥n Ä‘á» thá»±c sá»± cá»§a Small Model khÃ´ng pháº£i lÃ  "khÃ´ng muá»‘n" tuÃ¢n thá»§, mÃ  lÃ  **thiáº¿u nÄƒng lá»±c xá»­ lÃ½** Ä‘á»ƒ Ä‘á»c, lá»c nhiá»…u, xá»­ lÃ½ mÃ¢u thuáº«n giá»¯a context vÃ  parametric knowledge, vÃ  tuÃ¢n thá»§ cÃ¡c instruction phá»©c táº¡p trong má»™t context RAG dÃ i. ChÃºng dá»… bá»‹ "overloaded" vÃ  sinh ra output kÃ©m cháº¥t lÆ°á»£ng.
    *   Do Ä‘Ã³, viá»‡c huáº¥n luyá»‡n RAG-SFT (Phase 2.5) lÃ  Ä‘á»ƒ **tÄƒng cÆ°á»ng kháº£ nÄƒng xá»­ lÃ½ context hiá»‡u quáº£** cho model, dáº¡y nÃ³ cÃ¡ch tÃ­ch há»£p thÃ´ng tin RAG vÃ o payload má»™t cÃ¡ch chÃ­nh xÃ¡c, Ä‘Ãºng cÃº phÃ¡p vÃ  tuÃ¢n thá»§ cÃ¡c rÃ ng buá»™c.

3.  **Káº¿t luáº­n:**
    *   Khi tÃ­ch há»£p model Phase 2/3 vÃ o há»‡ thá»‘ng khÃ¡c (vÃ­ dá»¥: RAG), **PHáº¢I** Ä‘áº£m báº£o xÃ¢y dá»±ng prompt Ä‘Ãºng cáº¥u trÃºc nhÆ° trong `scripts/build_phase2_dataset.py`.
    *   Viá»‡c performance tháº¥p Ä‘á»™t ngá»™t thÆ°á»ng do "Prompt Mismatch" hoáº·c "Context Overload" chá»© khÃ´ng pháº£i do model bá»‹ lá»—i hay cá»‘ tÃ¬nh bá» qua RAG.
